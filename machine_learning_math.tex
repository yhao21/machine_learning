
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{Math Camp for Machine Learning}
\author{Synferlo}
\date{Mar. 22, 2021}


\begin{document}
\maketitle
\newpage



\section{Statistical Learning}


\subsection{Elements in ML}

{\textbf {1. Instance/example:}}\\
$ x $, $ x \in X $\\


{\textbf {2. Instance space/domain:}}\\
$ X $ (where the instance comes from).\\


{\textbf {3. Label:}}\\
Each instance has a label, or class. Label can be 0/1 or +/-.\\


{\textbf {4. Concept:}}\\
There's a function $ c $, called concept, that tells the
{\textbf {true}} relationship between instance and label.
\begin{equation*}
				\text{ concept }c: X \rightarrow \left\{ 0,1 \right\} 
\end{equation*}
Each instance $ x $ is labeled by $ c(x) $.
{\underline {Our goal is to find this $ c(\cdot) $}}.\\


{\textbf {5. Hypothesis:}}\\
Note, this is {\underline {NOT}} the same one as we
say in econometrics. 
Hypothesis, $ h(\cdot ) $, is a function that the machine use to do the
{\underline {prediction}} given an instance $ x $.
\begin{equation*}
				h: X \rightarrow \left\{ 0, 1 \right\} 
\end{equation*}\\


{\textbf {6. Concept VS Hypothesis:}}\\
Concept is the {\underline {TRUE}} relationship between $ x $ and label.\\
Hypothesis is the {\underline {GUESS}} of our machine given the training data.\\


{\textbf {7. Concept class:}}\\
$ C $ is where concept $ c $ comes from, $ c \in C $.\\


{\textbf {8. Distribution:}}\\
All instances are generated from a particular distribution $ D $.
We call it {\underline {target distribution}} or distribution for short.
\begin{equation*}
				x_{i} \sim D, i.i.d.
\end{equation*}\\


{\textbf {Hypothesis class:}}\\
It tells where the hypothesis comes from.
We allow $ h $ and $ c $ come from different classes.
\begin{equation*}
				h \in \mathcal{H}
\end{equation*}\\



\newpage


\subsection{ML Process}

\begin{figure}[ht]
    \centering
    \incfig{ml-process}
    \caption{ML process}
    \label{fig:ml-process}
\end{figure}


\subsection{PAC Learning}

We want to see $ h(x) = c(x) $\\
We DO NOT want to see $ h(x) \ne c(x) $\\

\subsubsection{How we measure error:}

\begin{equation*}
				err_{D}(h) = Pr_{x \sim D} \left[ h(x) \ne c(x) \right] 
\end{equation*}

We want this,

\begin{equation*}
				err_{D}(h) \le \varepsilon ,
\end{equation*}
where $ \varepsilon  $ is a small positive number.

To guarantee the machine work well, we require the following condition,
\begin{equation*}
				Pr \left( err_{D}(h) \le \varepsilon  \right) \ge 1-\delta 
\end{equation*}
where $ \delta  $ is a small positive number.

Hence, $ err_{D} \le \varepsilon  $ requires algorithm to be more accurate.
$ Pr(err \le \varepsilon ) \ge 1 - \delta  $ requires the probability of
this correction to be {\underline {high}}.

This method is called {\underline {Probability approximately correct}},
or PAC for short.\\

================================================\\
We say concept space $ C $ is PAC-learnable by $ \mathcal{H} $,\\
if there exist an algorithm (alg.) $ A $, $ \forall c \in C $,
$ \forall  $ distribution $ D $, $ \forall \varepsilon > 0, \delta >0 $,\\
$ A $ takes $ m = poly(\frac{1}{\varepsilon }, \frac{1}{\delta },\cdots) $
random examples $ x_{i} \sim D $,\\
that it makes output hypothesis $ h \in \mathcal{H} \quad s.t.$ 
$ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.

NB: m is sample size. The more data we have, the higher accuracy that
$ h(\cdot ) $ will be. Hence, m is {\underline {negative correlated}} with
$ \varepsilon  $ and $ \delta  $.


================================================\\

\newpage
Here's an example:
For $ X \in {\rm I\!R} $
\begin{figure}[ht]
    \centering
    \incfig{threshold}
    \caption{Threshold}
    \label{fig:threshold}
\end{figure}


There are many instances on the real line. Concept $ c $ is a threshold Function.

All instances on the right are labeled by + (True)\\
All instances on the left are labeled by - (False)\\


What we are doing is like this,

\begin{figure}[ht]
    \centering
    \incfig{hyp.-and-concept}
    \caption{hyp. and concept}
    \label{fig:hyp.-and-concept}
\end{figure}


So, output $ h(x) $ would be
\begin{equation*}
				h(x) = 
				\begin{cases}
				+ \quad \text{ if } x \ge b\\
				- \quad \text{ O.W. }
				\end{cases}
\end{equation*}

In this case, we say $ \mathcal{H} = C $.


\newpage
Now, let's consider a general case.

\begin{figure}[ht]
    \centering
    \incfig{general-case}
    \caption{General case}
    \label{fig:general-case}
\end{figure}

If a training data example $ x_{i} $ falls in $ (c, c + \varepsilon ) $, region
R, we have to shift $ h(\cdot ) $ to the left of $ x_{i} $, so that
\begin{equation*}
				Pr \left[ \quad
				err_{D}(h) > \varepsilon \quad \right] \le 
				Pr \left( \quad
				\text{  no } x_{i} \text{ in } R \quad\right) 
				= 
				Pr \left( 
				x_1 \notin R, x_2 \notin R, \cdots x_{m}, \notin R\right) 
\end{equation*}

Since $ x_{i} $  is $ i.i.d. $, we can write
\begin{equation*}
				Pr (x_1 \notin R, \cdots, x_{m} \notin R)
				= \prod_{i = 1} ^ m Pr(x_{i} \notin R) = \prod_{i = 1} ^ m
				(1 - \varepsilon ) = (1 - \varepsilon )^{m}
\end{equation*}

Recall from basic calculus, $ 1 + x \le e^{x} $, so we can rewrite
\begin{equation*}
				(1 - \varepsilon )^{m} \le (e^{ - \varepsilon })^{m} 
				= e^{ - \varepsilon m}
\end{equation*}

Also, since
\begin{equation*}
				Pr (err_{D}(h) \le \varepsilon ) \ge 1 - \delta
\end{equation*}
we have
\begin{equation*}
				Pr (err_{D}(h) > \varepsilon ) \le \delta 
\end{equation*}


So, we end up with 
\begin{align*}
				Pr (err_{D}(h) > \varepsilon ) &\le e^{ - \varepsilon m}\\
				& \le \delta 
\end{align*}


We need to solve for m, so that we can know the condition for sample size.
Because we need to make sure this upper bound, $ e^{ - \varepsilon m} $,
no greater than $ \delta  $, we can write this,
\begin{align*}
				e^{ - \varepsilon m} &\le \delta \\
				 - \varepsilon m & \le \ln \delta \\
				m & \ge   - \frac{\ln \delta }{\varepsilon }\\
				m & \ge \frac{\ln \frac{1}{\delta }}{\varepsilon }
\end{align*}

It says at least you should have sample size greater than this lower bound
to guarantee $ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.












\end{document}

