
\documentclass[12pt]{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\newtheorem{thm}{Theorem}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{Math Camp for Machine Learning}
\author{Synferlo}
\date{Mar. 22, 2021}


\begin{document}
\maketitle
\newpage



\section{Statistical Learning}


\subsection{Elements in ML}

{\textbf {1. Instance/example:}}\\
$ x $, $ x \in X $\\


{\textbf {2. Instance space/domain:}}\\
$ X $ (where the instance comes from).\\


{\textbf {3. Label:}}\\
Each instance has a label, or class. Label can be 0/1 or +/-.\\


{\textbf {4. Concept:}}\\
There's a function $ c $, called concept, that tells the
{\textbf {true}} relationship between instance and label.
\begin{equation*}
				\text{ concept }c: X \rightarrow \left\{ 0,1 \right\} 
\end{equation*}
Each instance $ x $ is labeled by $ c(x) $.
{\underline {Our goal is to find this $ c(\cdot) $}}.\\


{\textbf {5. Hypothesis:}}\\
Note, this is {\underline {NOT}} the same one as we
say in econometrics. 
Hypothesis, $ h(\cdot ) $, is a function that the machine use to do the
{\underline {prediction}} given an instance $ x $.
\begin{equation*}
				h: X \rightarrow \left\{ 0, 1 \right\} 
\end{equation*}\\


{\textbf {6. Concept VS Hypothesis:}}\\
Concept is the {\underline {TRUE}} relationship between $ x $ and label.\\
Hypothesis is the {\underline {GUESS}} of our machine given the training data.\\


{\textbf {7. Concept class:}}\\
$ C $ is where concept $ c $ comes from, $ c \in C $.\\


{\textbf {8. Distribution:}}\\
All instances are generated from a particular distribution $ D $.
We call it {\underline {target distribution}} or distribution for short.
\begin{equation*}
				x_{i} \sim D, i.i.d.
\end{equation*}\\


{\textbf {Hypothesis class:}}\\
It tells where the hypothesis comes from.
We allow $ h $ and $ c $ come from different classes.
\begin{equation*}
				h \in \mathcal{H}
\end{equation*}\\



\newpage


\subsection{ML Process}

\begin{figure}[ht]
    \centering
    \incfig{ml-process}
    \caption{ML process}
    \label{fig:ml-process}
\end{figure}


\subsection{PAC Learning}

We want to see $ h(x) = c(x) $\\
We DO NOT want to see $ h(x) \ne c(x) $\\

\subsubsection{How we measure error:}

\begin{equation*}
				err_{D}(h) = Pr_{x \sim D} \left[ h(x) \ne c(x) \right] 
\end{equation*}

We want this,

\begin{equation*}
				err_{D}(h) \le \varepsilon ,
\end{equation*}
where $ \varepsilon  $ is a small positive number.

To guarantee the machine work well, we require the following condition,
\begin{equation*}
				Pr \left( err_{D}(h) \le \varepsilon  \right) \ge 1-\delta 
\end{equation*}
where $ \delta  $ is a small positive number.

Hence, $ err_{D} \le \varepsilon  $ requires algorithm to be more accurate.
$ Pr(err \le \varepsilon ) \ge 1 - \delta  $ requires the probability of
this correction to be {\underline {high}}.

This method is called {\underline {Probability approximately correct}},
or PAC for short.\\

================================================\\
We say concept space $ C $ is PAC-learnable by $ \mathcal{H} $,\\
if there exist an algorithm (alg.) $ A $, $ \forall c \in C $,
$ \forall  $ distribution $ D $, $ \forall \varepsilon > 0, \delta >0 $,\\
$ A $ takes $ m = poly(\frac{1}{\varepsilon }, \frac{1}{\delta },\cdots) $
random examples $ x_{i} \sim D $,\\
that it makes output hypothesis $ h \in \mathcal{H} \quad s.t.$ 
$ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.

NB: m is sample size. The more data we have, the higher accuracy that
$ h(\cdot ) $ will be. Hence, m is {\underline {negative correlated}} with
$ \varepsilon  $ and $ \delta  $.


================================================\\

\newpage
Here's an example:
For $ X \in {\rm I\!R} $
\begin{figure}[ht]
    \centering
    \incfig{threshold}
    \caption{Threshold}
    \label{fig:threshold}
\end{figure}


There are many instances on the real line. Concept $ c $ is a threshold Function.

All instances on the right are labeled by + (True)\\
All instances on the left are labeled by - (False)\\


What we are doing is like this,

\begin{figure}[ht]
    \centering
    \incfig{hyp.-and-concept}
    \caption{hyp. and concept}
    \label{fig:hyp.-and-concept}
\end{figure}


So, output $ h(x) $ would be
\begin{equation*}
				h(x) = 
				\begin{cases}
				+ \quad \text{ if } x \ge b\\
				- \quad \text{ O.W. }
				\end{cases}
\end{equation*}

In this case, we say $ \mathcal{H} = C $.


\newpage
Now, let's consider a general case.

\begin{figure}[ht]
    \centering
    \incfig{general-case}
    \caption{General case}
    \label{fig:general-case}
\end{figure}

If a training data example $ x_{i} $ falls in $ (c, c + \varepsilon ) $, region
R, we have to shift $ h(\cdot ) $ to the left of $ x_{i} $, so that
\begin{equation*}
				Pr \left[ \quad
				err_{D}(h) > \varepsilon \quad \right] \le 
				Pr \left( \quad
				\text{  no } x_{i} \text{ in } R \quad\right) 
				= 
				Pr \left( 
				x_1 \notin R, x_2 \notin R, \cdots x_{m}, \notin R\right) 
\end{equation*}

Since $ x_{i} $  is $ i.i.d. $, we can write
\begin{equation*}
				Pr (x_1 \notin R, \cdots, x_{m} \notin R)
				= \prod_{i = 1} ^ m Pr(x_{i} \notin R) = \prod_{i = 1} ^ m
				(1 - \varepsilon ) = (1 - \varepsilon )^{m}
\end{equation*}

Recall from basic calculus, $ 1 + x \le e^{x} $, so we can rewrite
\begin{equation*}
				(1 - \varepsilon )^{m} \le (e^{ - \varepsilon })^{m} 
				= e^{ - \varepsilon m}
\end{equation*}

Also, since
\begin{equation*}
				Pr (err_{D}(h) \le \varepsilon ) \ge 1 - \delta
\end{equation*}
we have
\begin{equation*}
				Pr (err_{D}(h) > \varepsilon ) \le \delta 
\end{equation*}


So, we end up with 
\begin{align*}
				Pr (err_{D}(h) > \varepsilon ) &\le e^{ - \varepsilon m}\\
				& \le \delta 
\end{align*}


We need to solve for m, so that we can know the condition for sample size.
Because we need to make sure this upper bound, $ e^{ - \varepsilon m} $,
no greater than $ \delta  $, we can write this,
\begin{align*}
				e^{ - \varepsilon m} &\le \delta \\
				 - \varepsilon m & \le \ln \delta \\
				m & \ge   - \frac{\ln \delta }{\varepsilon }\\
				m & \ge \frac{\ln \frac{1}{\delta }}{\varepsilon }
\end{align*}

It says at least you should have sample size greater than this lower bound
to guarantee $ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.



\subsection{Finite Hypothesis Space}

Let's start with finite hypothesis space case, $ \left\lvert 
\mathcal{H} \right\rvert < \infty  $.

============================================\\
\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{thm}
    
    \end{thm}


Suppose hypothesis space $ \mathcal{H} $ is finite, Algorithm $ A $ finds
hypothesis $ h_{A} \in \mathcal{H} $ is consistent with $ m $ random 
training examples where
\begin{equation}
				m \ge \frac{1}{\varepsilon }(\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta })
\end{equation}

Then we can say 
\begin{equation}
				Pr \left[ 
				err_{D}(h_{A}) > \varepsilon \right] 
				\le \delta 
\end{equation}

or it is PAC learnable.


Equivalently, we can write,
\begin{align*}
				&\text{with } prob. \ge 1 - \delta,\\
				&err_{D}(h) \le  \frac{\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta }}{m} = \varepsilon 
\end{align*}
where $ h $ is a random variable.\\

    }%
}\\


================================\\
Notice, {\underline {consistent}} here is NOT what we mean in Econometrics!!\\
Here, consistent means {\underline {hyp.}} makes NO mistake in training examples.

================================


In last section we have 
\begin{equation*}
				m \ge  \frac{\ln \frac{1}{\delta }}{\varepsilon }.
\end{equation*}

Now, we add $ \ln \left\lvert \mathcal{H} \right\rvert  $ to the RHS,
\begin{equation*}
				m \ge  \frac{1}{\varepsilon }
				(\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta })
\end{equation*}

Term $ \ln \left\lvert \mathcal{H} \right\rvert  $ measures the 
{\underline {complexity}} of the hypothesis space.\\

================================\\
Complexity:\\
If we want to give each hyp. a name in $ \mathcal{H} $, how many bits we need
to do that?

The number of bits you need would be $ log_{2}\left\lvert \mathcal{H}
\right\rvert  $, where $ \left\lvert \mathcal{H} \right\rvert  $ stands for
the {\underline {number of hypothesis}} in this class.
So, here, we use $ \ln \left\lvert \mathcal{H} \right\rvert  $ roughly
measures the complexity of $ \mathcal{H} $.

================================\\

Recall three conditions we need to do machine learning:
\begin{enumerate}
        \item enough data
				\item fit the training set well
				\item use simple classifier
\end{enumerate}

Equation (2) gives us an accurate classifier. Equation (1) tells us we have
enough data.
Remember, Alg. $ A $ finds a $ h_{A} $ is consistent with training data.
And $ \ln \left\lvert \mathcal{H} \right\rvert  $ bound the complexity.
The more complex it is, the higher value would $ \ln \left\lvert 
\mathcal{H}\right\rvert  $ be. Then, $ m $ will also go up. It means with
more complex hypothesis space, we need more data to get an accuracy prediction.
So, {\underline {we need to limit the complexity}}.
Note, $ \left\lvert \mathcal{H} \right\rvert  $ is the cardinality, not 
absolute value.\\


\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{thm}
    
    \end{thm}
    
Assume given $ m \ge \frac{1}{\varepsilon }(
\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta }) $ examples,
with $ prob. \ge 1 - \delta  $, 

$ \forall h \in \mathcal{H}: $\\
if $ h $ is consistent, then $ err_{D}(h) \le \varepsilon  $.

Note, 

if $ err_{D}(h) \le \varepsilon  $, we say $ h $ is $ \text{ 
$ \varepsilon  $-good} $

if $ err_{D}(h) \le \varepsilon  $, we say $ h $ is $ \text{ 
$ \varepsilon  $-bad} $

    }%
}\\


Proof:

$ Pr(\forall h \in \mathcal{H}: h \text{ consistent } \Rightarrow
h \text{ is } \text{ $ \varepsilon  $-good}) \ge 1 - \delta  $

can be written as 

$ Pr(\exists h \in \mathcal{H}: h \text{ $ cons. $ and $ h $
is $ \varepsilon  $-bad} ) \le \delta  $. 

================================\\
Remember $ h $ is a random variable
because it depends on training samples. But ``$ e $ is $ \varepsilon  $-bad"
is not a RV.

================================

Let's go back to the proof.

Define
\begin{equation*}
				B = \left\{ h \in \mathcal{H}: \text{ $ h $ is $ \varepsilon  $-bad }
				\right\} 
\end{equation*}
So we can write
\begin{align}
				&Pr(\exists h \in \mathcal{H}: \text{ $ h $ cons. } \Rightarrow
				\text{ $ h $ is $ \varepsilon  $-bad })\\
				=& Pr(\exists h \in B: h \text{ is cons. })\\
				=& Pr \left( \bigcup _{h \in B} (h \text{ is cons. })  \right) \\
				=& (h_1 \in B \text{ cons. }) \cup (h_2 \in B \text{ cons. })\cdots
				   (h_{n} \in B \text{ cons. })\\
				\le & \sum\limits_{h \in B} Pr (h \text{ cons. })
				\quad \text{ recall, }
				Pr(a \cup b) \le Pr(a) + Pr(b)
\end{align}

Then what is the $ Pr(h \text{ cons. }) $?

Because $ x_{i} $ are $ i.i.d. $,
\begin{align}
				Pr(h \text{ cons. })
				 &= Pr(h(x_1) = c(x_1)\cap \cdots \cap h(x_{m}) = c(x_{m}))\\
				 &= \prod_{i = 1} ^ m Pr(h(x_{i}) = c(x_{i}))\\
				 & \le  (1 - \varepsilon )^{m}
\end{align}
Note, since $ Pr(h(x_{i}) \ne c(x_{i})) \le \varepsilon  $, so
$ Pr(h(x_{i}) = c(x_{i})) \le 1 - \varepsilon  $.

Hence, from equation (7) and (10), we can write

\begin{align*}
				&Pr \left( 
				\bigcup _{h \in B} (h \text{ cons. }) \right) \\
				& \le \sum\limits_{h \in B} Pr (h \text{ cons. })\\
				& \le \left\lvert B \right\rvert (1 - \varepsilon )^{m} \quad
				\text{ Note, }
				\left\lvert B \right\rvert \le \left\lvert \mathcal{H} \right\rvert \\ 
				& \le \left\lvert \mathcal{H} \right\rvert e^{ - \varepsilon m}\\
				& \le  \delta 
\end{align*}


How do get $ \delta  $ from $ \left\lvert \mathcal{H} \right\rvert 
e^{ - \varepsilon m}$?\\
Take log, we have $ \ln \left\lvert \mathcal{H} \right\rvert  - 
\varepsilon m$.
Given $ m \ge \frac{1}{\varepsilon }(\ln \left\lvert \mathcal{H} \right\rvert ) $
\begin{align*}
				em & \ge \ln \left\lvert \mathcal{H} \right\rvert  + \ln 
				\frac{1}{\delta }\\
				- \varepsilon m & \le - \ln \left\lvert \mathcal{H} \right\rvert 
				 - \ln \frac{1}{\delta } =  - \ln \left\lvert \mathcal{H} \right\rvert 
				  + \ln \frac{1}{\delta }\\
				\ln \left\lvert \mathcal{H} \right\rvert   - \varepsilon m
				& \le \ln \delta \\
				\left\lvert \mathcal{H} \right\rvert e^{ - \varepsilon m}
				& \le \delta  \quad \text{ take exponential }\\
				&Q.E.D.
\end{align*}

\subsection{Infinite hypothesis space}


Suppose we have four examples on the real line. $ h_1 $ and $ h_2 $ give us
the same prediction.
\begin{figure}[ht]
    \centering
    \incfig{same-prediction}
    \caption{same prediction}
    \label{fig:same-prediction}
\end{figure}


The prediction would be different if we do this,

\begin{figure}[ht]
    \centering
    \incfig{different-hyp.}
    \caption{different hyp.}
    \label{fig:different-hyp.}
\end{figure}


For each of the four instances, hypothesis's behavior can be either labeling
instance as + or -. So, for $ m $ instances, the max behavior is $ 2^{m} $.
Here we have $ m + 1 $, 5, hypothesis.

\subsubsection{Growth Function}

A set of examples $ \mathcal S = \left\{ x_1, \cdots, x_{m} \right\}  $ contains
$ m $ labeled instances. For particular hypothesis $ h $,
\begin{equation}
				\Pi _{\mathcal{H}}(\mathcal S) = \left\{ 
				\left\{ h(x_1), \cdots, h(x_{m}) \right\} : h \in \mathcal{H} \right\} 
\end{equation}
where $ \Pi _{\mathcal{H}}(\mathcal S) $ is a set of hypothesis behaviors.

We can look at how bad (maximum number) this set can be.
\begin{equation*}
				\Pi _{H}(m) = \max_{\substack{\left\lvert \mathcal S
				\right\rvert = m \\}}
				\left\lvert \Pi _{\mathcal{H}}(\mathcal S) \right\rvert 
\end{equation*}
These are all cardinality, not absolute value. They measures the size.

The Growth function tells us the maximum number (the worst case) of labeling
behavior a hypothesis space can make given a sample with size $ m $.

In figure 6, we have $ m + 1 $ hypothesis to make it an effective hypothesis
space. And $ \Pi _{\mathcal{H}}(m) $ is called the growth function.


Recall, when $ \left\lvert \mathcal{H} \right\rvert  < \infty  $ , we have
\begin{equation*}
				err_{D}(h) \le \frac{\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta }}{m}
\end{equation*}

Now, we relax this finite assumption, and we can use the growth function 
to measure the complexity of the hypothesis space.\\


\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{thm}
    
    \end{thm}

		Given $ m $ training examples, with $ prob. \ge 1 - \delta  $,
		$ \forall h \in \mathcal{H} $, 

		if $ h $ is consistent, then
		\begin{equation}
						err_{D}(h) \le 
						O \left( \frac{\ln \Pi _{\mathcal{H}}(2m) + \ln \frac{1}{\delta }}
						{m} \right) 
		\end{equation}
    }%
}\\



If the growth function is polynomial (this is a nice case),
\begin{equation*}
				\Pi _{\mathcal{H}}(m) = O(m^{d})
\end{equation*}
where $ d $ is a constant. We use big O here to hide constant terms.
Hence the error becomes,
\begin{equation}
				err_{D}(h) \le \frac{d \ln m  + \ln \frac{1}{\delta }}{m}
\end{equation}


\noindent\fbox{%
    \parbox{\textwidth}{%
						In the nice case, 
						\begin{equation*}
										 \Pi _{\mathcal{H}}(m) = O(m^{d}),
						\end{equation*}
						it is the finite case.

						In the worst case, the growth function realizes all possible
						behaviors,
						\begin{equation*}
										\Pi _{H}(m) = 2^{m},
						\end{equation*}
						it is an infinite case.

    }%
}\\


For any hypothesis space $ \mathcal{H} $, it will be {\underline {either}} the
{\underline {nice case}}, or {\underline {the worst case}}. There's NO other
cases!




In the nice case, {\underline {learning is possible}} because we can solve
the $ err_{D}(h) $ according to the theorem.\\
In the worst case, {\underline {learning is not possible}}.

{\textbf {NB:}} \\
In $ \Pi _{H}(m) = O(m^{d}) $, $ d $ is called the VC-dimension, where
VC is the short form of Vapnik-Chervnenkis.

Before we introduce VC-dimension, we need to know {\underline {shattering}}
first. \\

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Shattering:}}

		Sample $ \mathcal S $ with size $ m $ is shattered by $ \mathcal{H} $, 
		if all behaviors are possible,
		$ \left\lvert \Pi _{H}(\mathcal S) \right\rvert = 2^{m}  $.

		Equivalently, we say sample $ \mathcal S $ is shattered by $ \mathcal{H} $,
		if $ \mathcal{H} $ can dichotomize all elements in sample $ \mathcal S $,
		i.e., $ \Pi _{H}(\mathcal S) = 2^{m} $.
    }%
}\\


\newpage

Here's an example

\begin{figure}[ht]
    \centering
		\incfig{shattering-example}
    \caption{shattering example}
    \label{fig:shattering-example}
\end{figure}

In a three-point example, sample is not shattered because we {\underline {cannot
}} label them using one threshold. Hence, we can shatter two points,
but we can never shatter three points.\\

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Definition:}}

		The VC-dimension for $ \mathcal{H} $ is the maximum size of a sample,
		which can be shattered by $ \mathcal{H} $.
		\begin{equation*}
						\text{ VC-dim }(\mathcal{H}) = 
						\max_{\substack{\\}} \left\{ m: \Pi _{\mathcal{H}}(m) = 2^{m}
						\right\} 
		\end{equation*}
    }%
}\\

In this case, VC-dim(intervals) = 2 because we can only shatter a two-point
sample.


\newpage

Extension:

Linear threshold function (LTF) in $ {\rm I\!R}^{n}  $ looks like this.
The VC-dim(LTF in $ {\rm I\!R}^{n} $) = $ n + 1 $

\begin{figure}[ht]
    \centering
    \incfig{linear-threshold-function}
    \caption{Linear threshold function}
    \label{fig:linear-threshold-function}
\end{figure}


\newpage

VC-dim(LTF that go through the origin) = $ n $
\begin{figure}[ht]
    \centering
    \incfig{ltf-go-through-the-origin}
    \caption{LTF go through the origin}
    \label{fig:ltf-go-through-the-origin}
\end{figure}



For finite hypothesis space $ \mathcal{H} $, 
\begin{equation*}
				\text{ VC-dim }(\mathcal{H}) \le \ln \left\lvert \mathcal{H}
				\right\rvert 
\end{equation*}

where $ \ln \left\lvert \mathcal{H} \right\rvert  $ measures the complexity
of the hypothesis space.





\newpage

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Summary:}}
		
		Given $ m $ training examples, with $ prob. \ge 1 - \delta  $,
		$ \forall h \in \mathcal{H} $:

		If $ h $ is consistent, then

    \begin{align}
						& err_{D}(h) \le \frac{
						\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta }}
						{m}, \quad
						\text{ if }\left\lvert \mathcal{H} \right\rvert < \infty
						\\
						& err_{D}(h) \le 
						O \left( \frac{\ln \Pi _{\mathcal{H}}(2m) + \ln \frac{1}{\delta }}
						{m} \right) , \quad
						\text{ for all } \mathcal{H}\text{ not only finite ones, }
    \end{align}


    }%
}\\


\subsubsection{Sauer's Lemma}

Review on textbook page 277

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Sauer's Lemma:}}
		
		Given d = VC-dim $ (\mathcal{H}) $,
		\begin{equation}
						\Pi _{\mathcal{H}}(m) \le \sum\limits_{i = 0} ^d \binom{m}{i}
						\le \left( \frac{em}{d} \right) ^{d}, \quad \text{ 
						if $ m \ge d \ge 1 $}
		\end{equation}
		where we can solve binomial term using
		\begin{equation*}
						\binom{m}{i} = \frac{m!}{i!(m - i)!}
		\end{equation*}
    }%
}\\


Recall, for any $ \mathcal{H} $, either\\
$ \Pi _{H}(m) = 2^{m} \quad \forall  m$, this means $ d =  $ VC-dim is infinite.
The worst case

or

$ \Pi _{\mathcal{H}}(m) = O(m^{d}) $ for some constant $ d $.
This is the nice case. It says $ d =  $VC-dim is finite.


If we plug equation (16) into (15),
\begin{align*}
				err_{D}(h) & \le O \left( 
				\frac{\ln \Pi _{\mathcal{H}} + \ln \frac{1}{\delta }}{m}\right) \\
				& \le O \left( 
				\frac{\ln \left( \frac{em}{d} \right)^{d}  + \ln \frac{1}{\delta } }{m
        }\right) \\
				& \le O \left( 
				\frac{d \ln \frac{m}{d} + d + \ln \frac{1}{\delta }}{m}\right)
\end{align*}

So, now, the VC-dim $ =d $ acts like a measure of complexity.

{\textbf {NB:}} the VC-dim also gives us a lower bound of how many examples
we need for training.

================================\\
Note, VC-dim$ (\mathcal{H} )= d $ means that there exists ($ \exists  $) a
sample with size $ d $ which can be shattered by hypothesis space $ \mathcal{H}$.
Here, we say there exists ($ \exists  $). Hence, it does NOT mean that for all
sample with size $ d $ can be shattered. One is enough.

================================


\subsubsection{Empirical Risk Minimization Model (ERM)}

There might be some randomness between today's weather and tomorrow's weather.
So, in stead of write $ y $ as a direct function of $ x $, $ y = f(x) $, 
we say instance $ x $ and label $ y $ are in a {\underline {pair}} 
$ (x, y) $. And this pair is followed a distribution $ D $, $ (x, y) \sim D $.

Hence, the measure of hypothesis $ h(\cdot ) $ becomes
\begin{equation*}
				err_{D}(h) = Pr_{(x,y) \sim D} [h(x) \ne y]
\end{equation*}

Now the problem becomes this:

Given sample $ (x_1,y_1), \cdots, (x_{m},y_{m}) $ where $ (x_{i},y_{i})
\sim D$.\\
We want: $ \min_{\substack{h \in \mathcal{H}\\}} err_{D}(h)  $.

This says we want to minimize the error, $ err_{D}(h) $ over all hypothesis in
$ \mathcal{H} $.


Notice, $ err_{D}(h) $ is the generalized error. Our {\textbf {training error}}
from the machine would be
\begin{equation*}
				\widehat{err_{D}(h)} = \frac{1}{m} \sum\limits_{i = 1} ^m
				1 \left\{ h(x_{i} \ne y_{i}) \right\} 
\end{equation*}
where $ 1 \left\{ h(x_{i} \ne y_{i}) \right\} $ is an indicator function.
We can write it in ECON way:
\begin{equation}
				\widehat{err_{D}(h)} = \frac{1}{m}\sum\limits_{i = 1} ^m
				I, \quad
				I = 
				\begin{cases}
								1 \quad \text{ if } h(x_{i}) \ne y_{i}\\
								0 \quad o.w.
				\end{cases}
\end{equation}

Clearly, $ \frac{1}{m}\sum\limits_{i} I	 $ is the sample mean of the indicator
function. It says in what percent the outcome hypothesis is wrong. Remember
$ I = 1 $ when the outcome $ h(x_{i}) $ is NOT the same as true label $ y_{i} $.

Hence, $ \widehat{err_{D}(h)} < 1 $.

And the hypothesis we get from the training machine would be
\begin{equation}
				\widehat{h} = \argmin_{h \in \mathcal{H}} err_{D}(h)
\end{equation}

This method is called {\underline {empirical risk minimization}}, or ERM for
short. The error term $ \widehat{err_{D}(h)} $ is called {\underline {
empirical risk}}.



So what we what to prove is the following theorem.

\noindent\fbox{%
    \parbox{\textwidth}{%
				\begin{thm}
				
				\end{thm}
				For sufficiently large $ m $, with $ prob. \ge 1 - \delta  $, 
				$ \forall h \in \mathcal{H} $:
				\begin{equation*}
								\left\lvert 
												err_{D}(h) - \widehat{err_{D}(h)}
								\right\rvert 
								\le \varepsilon , \quad \text{ here is abs rather cardinality. }
				\end{equation*}


    }%
}\\

It says that the training error is always close to the generalization error
for all hypothesis in $ \mathcal{H} $.

This is called {\underline {the uniform convergence result}}.
If the $ h(\cdot ) $ is consistent, then $ \widehat{err_{D}(h)} = 0 $, 
and $ err_{D}(h) $ is a small number.


Recall,
\begin{equation*}
				err_{D}(h) = Pr (h(x) \ne y)
\end{equation*}

The indicator function appears in equation (17) can be written in this way
\begin{equation*}
				I = 
				\begin{cases}
				1 \quad \text{ with } prob. = err_{D}(h)\\
				0 \quad o.w.
				\end{cases}
\end{equation*}

And the training error is the sample mean,
\begin{equation*}
				\widehat{err_{D}(h)} = \frac{1}{m} \sum\limits_{i = 1} ^m I	
\end{equation*}
So, we can prove how fast $ \widehat{err_{D}(h)} $ converge to $ err_{D}(h) $.\\

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Example:}}

		Given RVs, $ Z_1, \cdots, Z_{m}, \quad i.i.d.$, $ Z_{i} \in \left\{ 0,1
		\right\}  $.
		Let $ p = E(Z_{i}) $, expectation. We want to estimate $ p $ by
		$ \widehat{p} = \frac{1}{m}\sum\limits_{i} Z_{i}	 $, which is the
		sample mean.
		And the {\underline {Hoeffing inequality}} tells us (Hoeffding, 1963)
		\begin{align}
						Pr \left( 
						\frac{1}{m}\sum\limits_{i = 1} ^m x_{i} - \frac{1}{m}
		\sum\limits_{i = 1} ^m E(x_{i}) \ge \varepsilon 	\right) &\le 
		exp( - 2m \varepsilon ^{2})\\
		Pr \left( 
		\left\lvert \frac{1}{m}\sum\limits_{i = 1} ^m x_{i} - \frac{1}{m}
		\sum\limits_{i = 1} ^m E(x_{i})	\right\rvert \ge \varepsilon  \right) 
		& \le 2 exp( - 2m \varepsilon ^{2})
		\end{align}
		
    Hence, in our example, we have
		\begin{equation*}
						Pr(\widehat{p} - p \ge \varepsilon ) \le exp( - 2m \varepsilon ^{2}),
		\end{equation*}	
		where $ m $ stands for sample size, $ \varepsilon \in \left( 0,1 \right)  $.
		
		So, we have the sample mean converge to the expectation,
		\begin{equation*}
						\frac{1}{m}\sum\limits_{i = 1} ^m	Z_{i} \rightarrow E(Z_{i})
		\end{equation*}

		Or we can write

		\begin{equation*}
		\frac{1}{m}\sum\limits_{i} Z_{i} \rightarrow E(\frac{1}{m}\sum\limits_{i} 
		Z_{i}	)	,
		\end{equation*}
		since 
		$ E(\frac{1}{m}\sum\limits_{i} Z_{i}	) = \frac{1}{m}\sum\limits_{i}
		E(Z_{i}) = E(Z_{i})$
		
    Note,\\
		$ \frac{1}{m}\sum\limits_{i} Z_{i}	 $ can be written as 
		$ f(Z_1,\cdots,Z_{m}) $, \\
		$ E(\frac{1}{m}\sum\limits_{i} Z_{i}	) $ can be written as
		$ E(f(Z_1,\cdots,Z_{m})) $.
		Hence, we have 
    \begin{equation*}
    f(Z_1,\cdots,Z_{m}) \rightarrow E(f(Z_1,\cdots,Z_{m}))
    \end{equation*}




    }%
}\\


\subsubsection{McDiarmid's Inequality (McDiarmid, 1989)}

Suppose:\\
1. $ f(Z_1,\cdots,Z_{m}) $ is real-valued.

2. Changing in $ Z_{i} $ will change $ f(\cdot ) $ by {\underline {at most}}
$ c_{i} $, i.e., $ \forall Z_1,\cdots,Z_{m} $, $ Z_{i}' $


\begin{equation}
\left\lvert 
f(Z_1, \cdots, Z_{i}, \cdots, Z_{m}) - f(Z_1, \cdots, Z_{i}', \cdots, Z_{m})
\right\rvert
\le c_{i}
\end{equation}


Note, for each $ Z_{i} $, we have a equation (21) and $ c_{i} $. If all $ Z_{i} $
are changed, then we will have $ m $ number of $ c_{i} $.

3. Instances $ Z_1,\cdots, Z_{m} $ are independent, but NOT necessarily
identical.

If ALL of the above conditions are hold, then we can prove this convergence
result,


\begin{equation}
Pr \left( 
f(Z_1,\cdots,Z_{m}) - E(f(Z_1,\cdots,Z_{m})) \ge \varepsilon 
\right) 
\le exp
\left( 
\frac{ - 2 \varepsilon ^{2}}{\sum\limits_{i = 1} ^m c_{i}^{2}	}
\right) 
\end{equation}


In our example,
\begin{equation*}
f(Z_1,\cdots,Z_{m}) = \frac{1}{m}\sum\limits_{i = 1} ^m Z_{i}	,
\quad Z_{i} \in \left\{ 0,1 \right\} 
\end{equation*}


So, $ c_{i} = \frac{1}{m} $ because $ Z_{i} $ is an indicator function. It
can be either 0 or 1. If one of $ Z_{i} $ change from 0 to 1, $ f(\cdot ) $
increases $ \frac{1}{m} $, vice versa.







\subsubsection{Proof for Theorem 4}

Recall, theorem 4 says that

For sufficiently large $ m $, with $ prob. \ge 1 - \delta  $, $ \forall 
h \in \mathcal{H}$:


\begin{equation*}
\left\lvert 
err_{D}(h) - \widehat{err_{D}(h)}
\right\rvert 
\le \varepsilon 
\end{equation*}


Let's prove an easier one first, saying $ \exists h \in \mathcal{H} $,
the upper inequality holds.\\
Here,


\begin{align*}
Z_{i} &= I, \quad \text{ where } 
I = 
\begin{cases}
1 \quad \text{ if }h(x_{i}) \ne y_{i}\\
0 \quad o.w.
\end{cases}\\
p  &= E(Z_{i}) = err_{D}(h)\\
\widehat{p} &= \widehat{err_{D}(h)} = \frac{1}{m}\sum\limits_{i = 1} ^m I	
\end{align*}


From Hoeffding's inequality,
for $ \left\lvert \mathcal{H} \right\rvert  < \infty $,


\begin{equation}
Pr \left(  \exists h \in \mathcal{H}: 
\left\lvert err_{D}(h) - \widehat{err_{D}(h)} \ge  \varepsilon  \right\rvert 
\right)
\le 2 \left\lvert \mathcal{H} \right\rvert 
exp \left(   - 2m \varepsilon ^{2} \right)
\end{equation}


where $ \left\lvert \mathcal{H} \right\rvert  $ is the cardinality of 
$ \mathcal{H} $.\\

\noindent\fbox{%
\parbox{\textwidth}{%

How do we derive equation (23):

\begin{align*}
LHS  &= Pr \left( 
\left(\left\lvert 
err_{D}(h_1) - \widehat{err_{D}(h_1)}
\right\rvert > \varepsilon  \right)
\cup \cdots \cup 
\left(\left\lvert 
err_{D}(h_{\left\lvert \mathcal{H} \right\rvert }) - \widehat{err_{D}(h_{
\left\lvert \mathcal{H} \right\rvert })}
\right\rvert > \varepsilon  \right)
\right) \\
& \le \sum\limits_{h \in \mathcal{H}} Pr
\left(
\left\lvert 
err_{D}(h) - \widehat{err_{D}(h)}
\right\rvert  
> \varepsilon 
\right) 
\end{align*}

From Hoeffding's inequality,
\begin{equation*}
Pr \left( 
\left\lvert 
err_{D}(h) - \widehat{err_{D}(h)}
\right\rvert 
\ge \varepsilon 
\right) 
\le 2 exp( - 2m \varepsilon ^{2})
\end{equation*}

Hence,
\begin{equation*}
\sum\limits_{h \in \mathcal{H}} Pr \left( 
\left\lvert 
err_{D}(h) - \widehat{err_{D}(h)}
\right\rvert 
> \varepsilon 
\right) 
\le 2 \left\lvert \mathcal{H} \right\rvert exp ( - 2m \varepsilon ^{2})
\end{equation*}

}%
}\\


Now, let $ \delta  $ equals to the RHS of equation (23), then we can solve 
for $ \varepsilon  $.

\begin{align*}
2 \left\lvert \mathcal{H} \right\rvert exp( - 2m \varepsilon ^{2}) &= \delta \\
exp( - 2m \varepsilon ^{2}) &= \frac{\delta}{2 \left\lvert \mathcal{H}
\right\rvert }\\
 - 2m \varepsilon ^{2} &= \ln \frac{\delta }{2} - \ln 
 \left\lvert \mathcal{H} \right\rvert \\
 \varepsilon ^{2} &= \frac{\ln \left\lvert \mathcal{H} \right\rvert 
  - \ln \frac{\delta }{2}}{2m}\\
\varepsilon  &= \sqrt { \frac{\ln \left\lvert \mathcal{H} \right\rvert 
 + \ln \frac{2}{\delta }}{2m}}\\
\varepsilon  &= O \left( 
\sqrt { \frac{\ln \left\lvert \mathcal{H} \right\rvert 
 + \ln \frac{1}{\delta }}{m}}
\right) 
\end{align*}


Recall, theorem 4, plug in the value of $ \varepsilon  $ in to 
$ \left\lvert err_{D}(h) - \widehat{err_{D}(h)} \right\rvert \le \varepsilon $.

\begin{equation*}
\left\lvert err_{D}(h) - \widehat{err_{D}(h)} \right\rvert
\le O \sqrt {
\frac{\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta }}{m}
}
\end{equation*}

Hence, we can write,

\begin{equation}
\widehat{err_{D}(h)}  - O \left(\sqrt { \frac{\ln \left\lvert \mathcal{H}
\right\rvert  + \ln \frac{1}{\delta }}{m}}
\right) \le err_{D}(h) \le \widehat{err_{D}(h)}
 + O \left(\sqrt { \frac{ \left\lvert \ln \mathcal{H} \right\rvert
 + \ln \frac{1}{\delta }
 }{m}}\right)
\end{equation}




\noindent\fbox{%
\parbox{\textwidth}{%

Clearly, the generalization error, $ err_{D}(h) $, is related to 
$ \widehat{err_{D}(h)}, \ln \left\lvert \mathcal{H} \right\rvert , m, $ and
$ \delta  $.

The thing is that we expect the $ \widehat{err_{D}(h)}$ {\underline {goes down}}
as we {\underline {increase}} the complexity of $ \left\lvert 
\mathcal{H}\right\rvert  $. Higher complexity of $ \mathcal{H} $ means
that we add more restrictions to the model. And our model would fit 
the {\underline {training data}} better. Note, if we complexity if too high
even though the machine fit the training data better, we would have 
{\underline {over fitting}} problem.

For $ err_{D}(h) $, as $ \left\lvert \mathcal{H} \right\rvert  $ goes up,
it decreases initially, because it is affected by the training error.
However, as $ \mathcal{H} $ getting much more complex, training error becomes
very small, and $ O(\cdot ) $ dominates the trend. Hence,  $ err_{D}(h) $
goes up. The graph is shown below.



}%
}\\


\begin{figure}[ht]
    \centering
    \incfig{complexity-of-hyp.-space-and-error-trend}
    \caption{Complexity of hyp. space and error trend}
    \label{fig:complexity-of-hyp.-space-and-error-trend}
\end{figure}



\subsubsection{Rademacher Complexity}
Another to measure the complexity is using Rademacher complexity.
Radermacher is a Germany mathematician.


Given sample space $ \mathcal S  = (x_1,y_1),\cdots,(x_{m},y_{m})$,

$ y_{i} \in \left\{ -1, +1 \right\}  $, $ y $ is label.\\
$ h: X \longmapsto \left\{  - 1,  + 1 \right\}  $\\
$ \widehat{err_{D}(h)} = \frac{1}{m}\sum\limits_{i = 1} ^m I	 $\\

Since indicator I can be either 0 or 1, we can rewrite it as
\begin{equation*}
I = \frac{1 - y_{i}h(x_{i})}{2}
\end{equation*}


\newpage
Let me show you why it make sense. Remember, $ h(x_{i}) $ is the prediction,
$ h_{x_{i}} \in \left\{  - 1,  + 1 \right\}  $.

If true label $ y_{i} = 1 $:\\

\begin{equation*}
I = \frac{1 - h(x_{i})}{2} \quad
\begin{cases}
\text{ if }  h(x_{i}) = 1 , h(x_{i}) = y_{i} ,  I = \frac{1 - 1}{2} = 0 \\
\text{ if }  h(x_{i}) = -1 , h(x_{i}) \ne y_{i} ,  I = \frac{1 - (-1)}{2} = 1 \\
\end{cases}
\end{equation*}





If true label $ y_{i} = -1 $:\\
\begin{align*}
I = \frac{1 + h(x_{i})}{2}\quad
\begin{cases}
\text{ if }  h(x_{i}) = 1, h(x_{i}) \ne y_{i}, I = \frac{1 + 1}{2} = 1 \\
\text{ if }  h(x_{i}) = -1, h(x_{i}) = y_{i}, I = \frac{1 - 1}{2} = 0 
\end{cases}
\end{align*}






It says, $ I = 
\begin{cases}
1 \quad \text{ if } h(x_{i}) \ne y_{i}\\
0 \quad o.w.
\end{cases}$\\

It is exactly the same thing! Genius!


So we can rewrite the training error as the following,

\begin{align*}
\widehat{err_{D}(h)} &= \frac{1}{m}\sum\limits_{i = 1} ^m I\\
 &= \frac{1}{m}\sum\limits_{i = 1} ^m \frac{1 - y_{i}h(x_{i})}{2}	\\
 &= \frac{1}{m}\sum\limits_{i = 1} ^m \frac{1}{2}(1 - y_{i}h(x_{i}))\\
 &= \frac{1}{m}\sum\limits_{i} \frac{1}{2}  -  \frac{1}{2m}\sum\limits_{i}
 y_{i}h(x_{i})\\
 &= \frac{1}{2} - \frac{1}{2m}\sum\limits_{i = 1} ^m y_{i}h(x_{i})	
\end{align*}


Now, we are using the {\underline {average of}} $ y_{i}h(x_{i}) $ to measure
how good a hypothesis $ h $ is.
Since
\begin{equation*}
\widehat{err_{D}(h)} = \frac{1}{2} - \frac{1}{2} \frac{1}{m}
\sum\limits_{i = 1} ^m y_{i}h(x_{i})
\end{equation*}

Our optimization problem becomes this,
\begin{equation*}
\argmin \widehat{err_{D}(h)} \Longrightarrow \argmax_{h \in \mathcal{H}}
\frac{1}{m} \sum\limits_{i = 1} ^m y_{i}h(x_{i})
\end{equation*}


================================\\
Since in real world data the label $ y_{i} $ in $ (x_{i},y_{i}) $ can be
affected by random noise, $ y_{i} $ in $ (x_{i},y_{i}) $ no longer be the
true label.
How do we deal with this problem?

Let $ \sigma_{i} $ be an assumed label (a RV), 

\begin{equation*}
\sigma _{i} = 
\begin{cases}
1 \quad \text{ with } prob. = \frac{1}{2}\\
-1 \quad \text{ with } prob. = \frac{1}{2}
\end{cases}
\end{equation*}


Then we can rewrite the problem as,
\begin{equation}
\sup_{\substack{ h \in \mathcal{H} }} \frac{1}{m}\sum\limits_{i =1} ^m
\sigma _{i}h(x_{i})
\end{equation}

Here we use SUP rather MAX because $ \mathcal{H} $ is {\underline {infinite}},
it is possible that we cannot find the maximum.

Now we take expectation for equation (25) w.r.t. $ \sigma  $
\begin{equation}
R = E_{\sigma }\left( \sup_{\substack{ h \in \mathcal{H} }}
\frac{1}{m} \sum\limits_{i = 1} ^m \sigma _{i}h(x_{i})	\right) 
\end{equation}
where $ R \in [0,1] $. Here, we use $ R $ to measure the complexity of 
$ \mathcal{H} $. 

If $ \left\lvert \mathcal{H} \right\rvert =1  $ then $ R = 0 $.\\
If $ \left\lvert \mathcal{H} \right\rvert =2^{m}  $
and $ \mathcal S $ can be shattered by $ \mathcal{H} $
then $ R = 1 $.\\

So, $ R $ would be closed to 1 if $ \mathcal{H} $ is very complicated.

================================\\
To see why R = 0 if $ \left\lvert \mathcal{H} \right\rvert = 1 $,

If $ \left\lvert \mathcal{H} \right\rvert =1 $ we no longer have SUP
\begin{align*}
R &= E_{\sigma }\left( \frac{1}{m}\sum\limits_{i = 1} ^m \sigma _{i}h(x_{i})
\right) \\
 &= \frac{1}{m}\sum\limits_{i} E_{\sigma }\sigma _{i}h(x_{i}),
 \quad  \sigma _{i} = 
\begin{cases}
1 \text{ with } prob. = \frac{1}{2}\\
-1 \text{ with } prob. = \frac{1}{2}
\end{cases}\\
\end{align*}

Hence,
\begin{align*}
E_{\sigma }\sigma _{i}h(x_{i}) &= 
\frac{1}{2}\cdot (1)\cdot h(x_{i}) + \frac{1}{2}\cdot (-1)\cdot h(x_{i})\\
 &= \frac{1}{2}h(x_{i}) - \frac{1}{2}h(x_{i})\\
 &= 0
\end{align*}

So, $ R = 0 $
================================\\
If $ \left\lvert \mathcal{H} \right\rvert = 2^{m} $, and $ \mathcal S $
can be shattered by $ \mathcal{H} $, then there would be one hypothesis that
make $ h(x_{i}) = \sigma _{i} $. This says that one hypothesis makes
$ R = 1 $ which is the maximum because all predictions are correct.

It makes sense because when we have {\underline {infinite}} number of
hypothesis, there would be {\underline {one}} who can fit all values.





\newpage
\subsection{Empirical and expected RC}

Note, from here, I use $ \mathbb{E} $ for expectation, 
and $ E $ for the error of prediction.



Recall, 
\begin{align*}
&h: X \rightarrow \left\{  - 1, 1 \right\} , h \in \mathcal{H}\\
&\mathcal S = \left\{ x_1,x_2,\cdots,x_{m} \right\} \\
&\mathbb{E}_{\sigma }
\left[ 
\sup_{\substack{ h \in \mathcal{H} }}
\frac{1}{m} \sum\limits_{i = 1} ^m \sigma _{i}h(x_{i})	
\right]\\ 
\end{align*}

We want to show: 
\begin{equation*}
\forall  h \in \mathcal{H},\quad   \widehat{err}(h) \rightarrow
err(h)
\end{equation*}


Now, let's have a new setting.

\begin{align*}
&f: Z \rightarrow {\rm I\!R}, \quad f \in F\\
&Z = \left\{ z_1, z_2, \cdots, z_{m} \right\} \\
& \widehat{R}_{Z}(F) = \mathbb{E}_{\bm{\sigma}} 
\left[
\sup_{\substack{ f \in F }} \frac{1}{m} \sum\limits_{i = i} ^m	
\sigma _{i}f(z_{i})
\right] 
\end{align*}

where $  \widehat{R}_{Z}(F) $ is called the empirical Rademacher
complexity, $ F $ stands for the hypothesis space, $ Z $ stands
for a sample set, and $ f $ stands for a hypothesis.


Now we want:\\
$ \forall  f \in F $:
\begin{equation*}
 \widehat{E}_{Z}(f) \rightarrow E(f)
\end{equation*}
where $  \widehat{E}_{Z}(f) = \frac{1}{m}
\sum\limits_{i = 1} ^m f(z_{i})	$, and
$ E(f) = E_{Z \sim D}(f(z)) $.


Since $  \widehat{R}_{Z}(F) $ is defined on a particular sample, 
but, normally, samples are chosen at random from a distribution.
So the expected value with respect to the random sample $ Z $
is called expected Rademacher Complexity (RC).

\begin{equation*}
R_{m}(F) = \mathbb{E}_{Z \subseteq \mathbb{Z}:|Z|=m}
\left[ 
 \widehat{R}_{Z}(F)
\right] 
\end{equation*}

\noindent\fbox{%
\parbox{\textwidth}{%
\begin{thm}
\end{thm}

For function space maps sample to the real number between 
0 and 1,  $ F: Z \rightarrow [0,1] $

$ Z = \left\{ z_1,\cdots,z_{m} \right\} \quad z_{i} \sim D, iid$, 
$ \forall f \in F $ and with $ prob. \ge 1 - \delta  $ we have
the following results:


\begin{align}
E(f(z)) & \le  \frac{1}{m}\sum\limits_{i = 1} ^m f(z_{i})  + 
2 R_{m}(F)  + O 
\left( 
\sqrt { \frac{\ln \frac{1}{\delta }}{m}} 
\right), \quad O(\cdot ) = \sqrt { \frac{\ln \frac{1}{\delta }}{2m}}\\
E(f(z))  & \le  \frac{1}{m} \sum\limits_{i = 1} ^m f(z_{i}) + 
2  \widehat{R}_{Z}(f)  +  O
\left( 
\sqrt { \frac{\ln \frac{1}{\delta }}{m}}
\right), \quad O(\cdot ) = 3\sqrt { \frac{\ln \frac{2}{\delta }}{2m}}
\end{align}

}%
}\\

where $  \widehat{E}_{Z}(f) = 
\frac{1}{m}\sum\limits_{i = 1} ^m f(z_{i})	 $, stands for empirical
error and $ E[f(z)] $ stands for the true error.


How to prove equation (27)?



The intuition is that we want to find the bound (the max difference)
between the true error and the empirical error. Hence, let's define
\begin{equation*}
\Phi (Z) = \sup_{\substack{ f \in F }} 
\left( 
E(f) -  \widehat{E}_{Z}(f)
\right) 
\end{equation*}

{\textbf {Step 1}}

Now we claim that the relationship between the true supreme difference
and the empirical supreme difference is the following,

with $ prob. \ge 1 - \delta  $
\begin{align*}
&\Phi (Z) \le \mathbb{E}_{Z}(\Phi (Z))  +  O
\left( 
\sqrt { \frac{\ln \frac{1}{\delta }}{m}}
\right) \\
& \Phi (Z): \text{ true supreme difference }\\
& \mathbb{E}_{Z}(\Phi (Z)): \text{ empirical supreme difference }\\
& \text{$ Z $ is a RV, since the samples are drawn from distribution
$ D $}
\end{align*}


How do we know this? 

Since $ f(\cdot ) $ will change at most $ \frac{1}{m} $, and 
$ E(f) $ is constant, so, the difference 
$ E(f) -  \widehat{E}_{Z}(f) $ will change at most
$ \frac{1}{m} $. This means that $ \sup_{\substack{  }}
\left( 
E(f) -  \widehat{E}_{Z}(f)
\right) $
will change at most $ \frac{1}{m} $. It says that
\begin{align*}
\Phi (Z) - \Phi (Z') &\le \frac{1}{m}\\
\left\lvert 
\Phi (Z) - \Phi (Z')
\right\rvert & \le \frac{1}{m}
\end{align*}

So from McDiarmid inequality, we can write
\begin{equation*}
\Phi (Z) \le \mathbb{E}_{Z}(\Phi (Z)) + O
\left( 
\sqrt { \frac{\ln \frac{1}{\delta }}{m}}
\right) 
\end{equation*}




{\textbf {Step 2}}


Since the empirical error, $  \widehat{E}_{Z}(f) $ is based on a
particular sample $ Z $, it is finite. But the true error, 
$ E(f) $ is based on all possible samples from the distribution
$ D $. Hence, $ E(f) $ if infinite. We need to work on this.

The idea is that we {\underline {replace}} $ E(f) $ by another 
empirical average from a second (imaginary) sample.


\begin{align*}
&\text{ sample we have: } Z = 
\left\{ z_1,\cdots,z_{m} \right\}, \text{ called real sample }\\
&\text{ imaginary sample: } Z' = 
\left\{ z_1',\cdots, z_{m}' \right\}, \text{ called ghost sample }
\end{align*}

where, $ Z, Z' \sim D, iid $.

So the idea is that we replace $ E(f) $ in 
$ \Phi (Z) = \sup_{\substack{ f \in F }} = 
\left( 
E(f) -  \widehat{E}_{Z}(f)
\right) $
by the empirical average from $ Z' $. Hence, we can write
\begin{align}
\mathbb{E}_{Z}(\Phi (Z)) &= \mathbb{E}_{Z}
\left[ 
\sup_{\substack{ f \in F }}
\left( 
E(f) -  \widehat{E}_{Z}(f)
\right) 
\right] \\
& \le \mathbb{E}_{Z, Z'}
\left[ 
\sup_{\substack{ f }}
\left( 
 \widehat{E}_{Z'}(f) -  \widehat{E}_{Z}(f)
\right) 
\right] 
\end{align}

We are going to prove this relationship!

Since $ z_{i} $ and $ z_{i}' $ are all drawn from $ D $, iid.
It will not change the result if we {\underline {swap}} them
between two samples. Hence we do a ``flip-coin" here. If Head,
we put $ z_{i} $ to $ Z' $ and $ z_{i}' $ to $ Z $; If Tail, we do
nothing. After this swap process we will receive two new samples
$ T $ and $ T' $

\noindent\fbox{%
\parbox{\textwidth}{%
Before swap:
\begin{align*}
& Z = \left\{ z_1,z_2,\cdots,z_{m} \right\} \\
& Z' = \left\{ z_1',z_2',\cdots,z_{m}' \right\} 
\end{align*}

After swap:
\begin{align*}
& T = \left\{ z_1',z_2,\cdots,z_{m} \right\} \\
& T' = \left\{ z_1,z_2',\cdots,z_{m}' \right\} 
\end{align*}


}%
}\\

Now we go back to our equation
\begin{equation*}
 \widehat{E}_{Z'}(f) -  \widehat{E}_{Z}(f) = 
 \frac{1}{m}\sum\limits_{i = 1} ^m \left( 
f(z_{i}') - f(z_{i})
 \right) 	
\end{equation*}


Since we swap some of the examples(instances), the upper equation
becomes this

\begin{equation}
 \widehat{E}_{T'}(f) -  \widehat{E}_{T}(f) = \frac{1}{m}
 \sum\limits_{i = 1} ^m
 \begin{cases}
 f(z'_{i}) - f(z_{i}) \text{ if no swap }\\
 f(z_{i}) - f(z'_{i}) \text{ if swap }
 \end{cases}
\end{equation}


To simplify this equation, we introduce and indicator function
$ \sigma _{i} $ where
\begin{equation*}
\sigma _{i} = 
\begin{cases}
1 \quad \text{ if no swap }\\
-1 \quad \text{ if swap }
\end{cases}
\end{equation*}

Note, $ \sigma _{i} $ here is not the same as the one we defined 
previously.


Hence we can rewrite equation(31),
\begin{equation*}
 \widehat{E}_{T'}(f) -  \widehat{E}_{T}(f) = 
 \frac{1}{m}\sum\limits_{i = 1} ^m	\sigma _{i}
(f(z'_{i}) - f(z_{i})), \quad
\sigma _{i} = 
\begin{cases}
1 \quad \text{ if no swap }\\
-1 \quad \text{ if swap }
\end{cases}
\end{equation*}

Hence, we can now rewrite equation(29),

\noindent\fbox{%
\parbox{\textwidth}{%

\begin{align}
\mathbb{E}_{Z}(\Phi (Z)) &= \mathbb{E}_{Z}\left[ 
\sup_{\substack{ f \in F }} \left( 
E(f) -  \widehat{E}_{Z}(f)\right) 
\right] \\
& \le \mathbb{E}_{Z, Z'}\left[ 
\sup_{\substack{ f }}
\left( 
				 \widehat{E}_{Z'}(f) -  \widehat{E}_{Z}(f)
\right) 
\right]\\
 &= \mathbb{E}_{Z,Z',\bm{\sigma} }
 \left[ 
				\sup_{\substack{ f }}
				\left( 
								\frac{1}{m}\sum\limits_{i = 1} ^m \sigma _{i}
								f(z'_{i}) - f(z_{i})
				\right) 
 \right]\\
 &= \mathbb{E}_{Z,Z',\bm{\sigma}}
 \left[ 
				 \sup_{\substack{ f }}
				 \left( 
								 \frac{1}{m}\sum\limits_{i = 1} ^m \sigma_{i}f(z'_{i})
								  + \frac{1}{m}\sum\limits_{i = 1} ^m ( - \sigma _{i})
									f(z_{i})
				 \right) 
 \right] \\
& \le \mathbb{E}_{Z,Z',\bm{\sigma}}
\left[ 
				\sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i = 1} ^m \sigma _{i}
				f(z'_{i}) + \sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i = 1} ^m
				( - \sigma _{i}) f(z_{i})
\right] \\
 &= \mathbb{E}_{Z',\bm{\sigma}}
 \left( 
				 \sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i} \sigma _{i}
				 f(z'_{i})
 \right)  + 
\mathbb{E}_{Z,\bm{\sigma}}
\left( 
				\sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i} ( - \sigma _{i})
				f(z_{i})
\right) \\
 &= 2R_{m}(F)
\end{align}




}%
}\\


Note, empirical Rademacher Complexity is 
\begin{equation*}
 \widehat{R}_{Z}(F) = \mathbb{E}_{\bm{\sigma}}
\left( 
				\sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i} \sigma _{i}
				f(z_{i})
\right) 
\end{equation*}

And the expected RC is this
\begin{equation*}
R_{m}(F) = \mathbb{E}_{Z}
\left( 
				 \widehat{R}_{Z}(F)
\right) 
=
\mathbb{E}_{Z,\bm{\sigma} }
\left( 
				\sup_{\substack{ f }}\frac{1}{m}\sum\limits_{i} \sigma _{i}
				f(z_{i})
\right) 
\end{equation*}


Hence, the first part in equation(37) is definitely an expected
RC, $ R_{m}(F) $. 

What about the second part?
We know
\begin{equation*}
\sigma_{i} = 
\begin{cases}
1 \quad \text{ with } prob. = \frac{1}{2}\\
-1 \quad \text{ with } prob. =  \frac{1}{2}.
\end{cases}
\end{equation*}

So, $  - \sigma_{i} $ and $ \sigma $ are the same thing. The have the 
same distribution, and do not affect the results.
Hence, the second part is also a RC, $ R_{m}(F) $.

And we have the result:
\begin{equation}
\mathbb{E}_{Z}(\Phi(Z)) \le 2R_{m}(F)
\end{equation}


Recall, 
$
\Phi(Z) = \sup_{\substack{ f }}
\left[ 
				E(f) -  \widehat{E}_{Z}(f)
\right] 
$

We have now proved 

\noindent\fbox{%
\parbox{\textwidth}{%
\begin{equation}
E(f(z)) \le \mathbb{E}_{Z}(f(z)) + 2R_{m}(F) + O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right) , \quad
O(\cdot )=
				\sqrt { \frac{\ln \frac{1}{\delta}}{2m}}
\end{equation}

or we can write in this way,

\begin{equation}
E(f(z)) \le \frac{1}{m}\sum\limits_{i = 1} ^m f(z_{i}) + 2R_{m}(F)+O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right) 
\end{equation}


}%
}\\


\newpage


Recall 
\begin{align*}
err_{D}(h) &\le    \widehat{err}_{D}(h) + \cdots\\
\text{where, } err_{D}(h)  &= E(I), \text{ true error }\\
 \widehat{err}_{D}(h) &=  \widehat{E}_{Z}(I), \text{ empirical error }
\end{align*}
where 
$ 
I = 
\begin{cases}
1 \quad \text{ if } h(x_{i}) \ne y_{i}\\
0 \quad \text{ o.w. }
\end{cases}
$

Then we have the following theorem

\noindent\fbox{%
\parbox{\textwidth}{%
\begin{thm}
\end{thm}

For hyp. space $ \mathcal{H}: X \rightarrow \left\{ -1, +1 \right\}  $,
given sample $ \mathcal S = \left\{ x_1,x_2,\cdots,x_{m} \right\}  $
where $ x_{i}\sim D, iid, x_{i} \in X $, 

$ \forall h \in \mathcal{H}, $ with $ prob. \ge 1 - \delta $ we have:

\begin{align}
err_{D}(h) &\le  \widehat{err}_{D}(h) + R_{m}(\mathcal{H}) + O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right), \quad
O(\cdot ) = \sqrt { \frac{\ln \frac{1}{\delta}}{2m}}\\
err_{D}(h) & \le   \widehat{err}_{D}(h) +  \widehat{R}_{\mathcal S}
(\mathcal{H}) + O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right), O(\cdot ) = 
3 \sqrt { \frac{\ln \frac{2}{\delta}}{2m}}
\end{align}


}%
}\\




{\underline {Proof:}}

For $ \mathcal{H} $, define a new sample $ Z = X  \times 
\left\{  - 1, + 1 \right\} $, then hyp. $ h(\cdot ) $ in $\mathcal{H} $
becomes
\begin{equation*}
l_{h}(z) = l_{h}(x,y) = I = 
\begin{cases}
1 \quad \text{ if } h(x_{i}) \ne y_{i}\\
0 \quad \text{ o.w. }
\end{cases}
\end{equation*}


Hence, we transfer $ \mathcal{H} $ of $ \left\{  - 1, 1 \right\}  $
to a function space 
$ 
\mathscr{L}_{\mathcal{H}} = 
\left\{
				l_{h}: h \in \mathcal{H}
\right\} ,\quad
\mathscr{L}_{\mathcal{H}} \in [0,1]
$

And from the definition of RC, we can write
\begin{align*}
 \widehat{R}_{Z}(\mathscr{L}_{\mathcal{H}})  &= 
\mathbb{E}_{\bm{\sigma}}
\left( 
				\sup_{\substack{ l_{h} \in \mathscr{L}_{\mathcal{H}} }}
				\frac{1}{m}\sum\limits_{i = 1} ^m \sigma_{i}l_{h}(x_{i},y_{i})
\right) ,\text{ where } \sigma_{i} = 
\begin{cases}
1 \quad \text{ with } prob. = \frac{1}{2}\\
 - 1 \quad \text{ with }prob. = \frac{1}{2}
\end{cases}\\
 &= \mathbb{E}_{\bm{\sigma}}
 \left( 
				 \sup_{\substack{ h \in \mathcal{H} }}\sum\limits_{i = 1} ^m
				 \sigma_{i} \frac{1 - y_{i}h(x_{i})}{2}
 \right) \\
 &= \frac{1}{2} \mathbb{E}_{\bm{\sigma}}
\left( 
				\frac{1}{m}\sum\limits_{i} \sigma_{i} + 
				\sup_{\substack{ h \in \mathcal{H} }} \frac{1}{m}
				\sum\limits_{i} ( - y_{i}\sigma_{i}h(x_{i}))	
\right) \\
 &= 0 + \frac{1}{2} \mathbb{E}_{\bm{\sigma}}
\left( 
				\sup_{\substack{ h \in \mathcal{H} }} \frac{1}{m} 
				\sum\limits_{i} ( - y_{i}\sigma_{i}h(x_{i}))
\right)\\
 &= \frac{1}{2} \mathbb{E}_{\bm{\sigma}}
 \left( 
				 \sup_{\substack{ h \in \mathcal{H} }}\frac{1}{m}
				 \sum\limits_{i} (\sigma_{i}h(x_{i}))	
 \right) \\
 &= \frac{1}{2}  \widehat{R}_{\mathcal S}(\mathcal{H})
\end{align*}


Remember,
\begin{equation*}
\sigma_{i} = 
\begin{cases}
1 \quad \text{ with } prob. = \frac{1}{2}\\
 - 1 \quad \text{ with }prob. = \frac{1}{2}
\end{cases}
\end{equation*}

So, $ \mathbb{E}(\sigma_{i}) = 0 $, and then
$ \frac{1}{2}\mathbb{E}_{\bm{\sigma}}
\left( 
				\frac{1}{m}\sum\limits_{i} \sigma_{i}
\right) =0
$

Also, $ y_{i} $ stands for the true label, and
\begin{equation*}
y_{i} = 
\begin{cases}
1\\
-1
\end{cases}
\end{equation*}

$  - y_{i} \sigma_{i} $ and $ \sigma_{i} $ have the same distribution.
So, we can replace $  - y_{i}\sigma_{i} $ with $ \sigma_{i} $.

{\underline {They are exactly the same thing!!}}

So far, we have 
\begin{equation*}
 \widehat{R}_{Z}(\mathscr{L}_{\mathcal{H}}) = \frac{1}{2}
  \widehat{R}_{\mathcal S}(\mathcal{H})
\end{equation*}
take expectation we obtain the expected Rademacher Complexity,

\noindent\fbox{%
\parbox{\textwidth}{%
\begin{equation}
R_{m}(\mathscr{L}_{\mathcal{H}}) = \frac{1}{2}R_{m}(\mathcal{H})
\end{equation}
}%
}\\



Recall from the theorem,
\begin{equation*}
E(f(x_{i})) \le  \widehat{E}_{\mathcal S}(f(x_{i})) + 2R_{m}(F) + O
\left( 
				\sqrt { \frac{\ln  \frac{1}{\delta}}{m}}
\right) 
\end{equation*}
In our case, 
\begin{align}
E(f(x_{i})) &\text{ stands for } err_{D}(h)\\
 \widehat{E}_{\mathcal S}(f(x_{i})) &\text{ stands for }
 \widehat{err}_{D}(h)\\
F & \text{ stands for } \mathscr{L}_{\mathcal{H}}
\end{align}

Hence we can write this,
\begin{equation*}
2R_{m}(F) = 2R_{m}(\mathscr{L}_{\mathcal{H}}) = 2  \times \frac{1}{2}
R_{m}(\mathcal{H}) = R_{m}(\mathcal{H})
\end{equation*}
So, we have proved the result:
\begin{align*}
err_{D}(h) & \le  \widehat{err}_{D}(h) + R_{m}(\mathcal{H}) + O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right) \\
\text{ or }\\
err_{D}(h) & \le  \widehat{err}_{D}(h) +  \widehat{R}_{\mathcal S}
(\mathcal{H}) + O
\left( 
				\sqrt { \frac{\ln \frac{1}{\delta}}{m}}
\right) 
\end{align*}




\end{document}

