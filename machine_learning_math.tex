
\documentclass[12pt]{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{Math Camp for Machine Learning}
\author{Synferlo}
\date{Mar. 22, 2021}


\begin{document}
\maketitle
\newpage



\section{Statistical Learning}


\subsection{Elements in ML}

{\textbf {1. Instance/example:}}\\
$ x $, $ x \in X $\\


{\textbf {2. Instance space/domain:}}\\
$ X $ (where the instance comes from).\\


{\textbf {3. Label:}}\\
Each instance has a label, or class. Label can be 0/1 or +/-.\\


{\textbf {4. Concept:}}\\
There's a function $ c $, called concept, that tells the
{\textbf {true}} relationship between instance and label.
\begin{equation*}
				\text{ concept }c: X \rightarrow \left\{ 0,1 \right\} 
\end{equation*}
Each instance $ x $ is labeled by $ c(x) $.
{\underline {Our goal is to find this $ c(\cdot) $}}.\\


{\textbf {5. Hypothesis:}}\\
Note, this is {\underline {NOT}} the same one as we
say in econometrics. 
Hypothesis, $ h(\cdot ) $, is a function that the machine use to do the
{\underline {prediction}} given an instance $ x $.
\begin{equation*}
				h: X \rightarrow \left\{ 0, 1 \right\} 
\end{equation*}\\


{\textbf {6. Concept VS Hypothesis:}}\\
Concept is the {\underline {TRUE}} relationship between $ x $ and label.\\
Hypothesis is the {\underline {GUESS}} of our machine given the training data.\\


{\textbf {7. Concept class:}}\\
$ C $ is where concept $ c $ comes from, $ c \in C $.\\


{\textbf {8. Distribution:}}\\
All instances are generated from a particular distribution $ D $.
We call it {\underline {target distribution}} or distribution for short.
\begin{equation*}
				x_{i} \sim D, i.i.d.
\end{equation*}\\


{\textbf {Hypothesis class:}}\\
It tells where the hypothesis comes from.
We allow $ h $ and $ c $ come from different classes.
\begin{equation*}
				h \in \mathcal{H}
\end{equation*}\\



\newpage


\subsection{ML Process}

\begin{figure}[ht]
    \centering
    \incfig{ml-process}
    \caption{ML process}
    \label{fig:ml-process}
\end{figure}


\subsection{PAC Learning}

We want to see $ h(x) = c(x) $\\
We DO NOT want to see $ h(x) \ne c(x) $\\

\subsubsection{How we measure error:}

\begin{equation*}
				err_{D}(h) = Pr_{x \sim D} \left[ h(x) \ne c(x) \right] 
\end{equation*}

We want this,

\begin{equation*}
				err_{D}(h) \le \varepsilon ,
\end{equation*}
where $ \varepsilon  $ is a small positive number.

To guarantee the machine work well, we require the following condition,
\begin{equation*}
				Pr \left( err_{D}(h) \le \varepsilon  \right) \ge 1-\delta 
\end{equation*}
where $ \delta  $ is a small positive number.

Hence, $ err_{D} \le \varepsilon  $ requires algorithm to be more accurate.
$ Pr(err \le \varepsilon ) \ge 1 - \delta  $ requires the probability of
this correction to be {\underline {high}}.

This method is called {\underline {Probability approximately correct}},
or PAC for short.\\

================================================\\
We say concept space $ C $ is PAC-learnable by $ \mathcal{H} $,\\
if there exist an algorithm (alg.) $ A $, $ \forall c \in C $,
$ \forall  $ distribution $ D $, $ \forall \varepsilon > 0, \delta >0 $,\\
$ A $ takes $ m = poly(\frac{1}{\varepsilon }, \frac{1}{\delta },\cdots) $
random examples $ x_{i} \sim D $,\\
that it makes output hypothesis $ h \in \mathcal{H} \quad s.t.$ 
$ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.

NB: m is sample size. The more data we have, the higher accuracy that
$ h(\cdot ) $ will be. Hence, m is {\underline {negative correlated}} with
$ \varepsilon  $ and $ \delta  $.


================================================\\

\newpage
Here's an example:
For $ X \in {\rm I\!R} $
\begin{figure}[ht]
    \centering
    \incfig{threshold}
    \caption{Threshold}
    \label{fig:threshold}
\end{figure}


There are many instances on the real line. Concept $ c $ is a threshold Function.

All instances on the right are labeled by + (True)\\
All instances on the left are labeled by - (False)\\


What we are doing is like this,

\begin{figure}[ht]
    \centering
    \incfig{hyp.-and-concept}
    \caption{hyp. and concept}
    \label{fig:hyp.-and-concept}
\end{figure}


So, output $ h(x) $ would be
\begin{equation*}
				h(x) = 
				\begin{cases}
				+ \quad \text{ if } x \ge b\\
				- \quad \text{ O.W. }
				\end{cases}
\end{equation*}

In this case, we say $ \mathcal{H} = C $.


\newpage
Now, let's consider a general case.

\begin{figure}[ht]
    \centering
    \incfig{general-case}
    \caption{General case}
    \label{fig:general-case}
\end{figure}

If a training data example $ x_{i} $ falls in $ (c, c + \varepsilon ) $, region
R, we have to shift $ h(\cdot ) $ to the left of $ x_{i} $, so that
\begin{equation*}
				Pr \left[ \quad
				err_{D}(h) > \varepsilon \quad \right] \le 
				Pr \left( \quad
				\text{  no } x_{i} \text{ in } R \quad\right) 
				= 
				Pr \left( 
				x_1 \notin R, x_2 \notin R, \cdots x_{m}, \notin R\right) 
\end{equation*}

Since $ x_{i} $  is $ i.i.d. $, we can write
\begin{equation*}
				Pr (x_1 \notin R, \cdots, x_{m} \notin R)
				= \prod_{i = 1} ^ m Pr(x_{i} \notin R) = \prod_{i = 1} ^ m
				(1 - \varepsilon ) = (1 - \varepsilon )^{m}
\end{equation*}

Recall from basic calculus, $ 1 + x \le e^{x} $, so we can rewrite
\begin{equation*}
				(1 - \varepsilon )^{m} \le (e^{ - \varepsilon })^{m} 
				= e^{ - \varepsilon m}
\end{equation*}

Also, since
\begin{equation*}
				Pr (err_{D}(h) \le \varepsilon ) \ge 1 - \delta
\end{equation*}
we have
\begin{equation*}
				Pr (err_{D}(h) > \varepsilon ) \le \delta 
\end{equation*}


So, we end up with 
\begin{align*}
				Pr (err_{D}(h) > \varepsilon ) &\le e^{ - \varepsilon m}\\
				& \le \delta 
\end{align*}


We need to solve for m, so that we can know the condition for sample size.
Because we need to make sure this upper bound, $ e^{ - \varepsilon m} $,
no greater than $ \delta  $, we can write this,
\begin{align*}
				e^{ - \varepsilon m} &\le \delta \\
				 - \varepsilon m & \le \ln \delta \\
				m & \ge   - \frac{\ln \delta }{\varepsilon }\\
				m & \ge \frac{\ln \frac{1}{\delta }}{\varepsilon }
\end{align*}

It says at least you should have sample size greater than this lower bound
to guarantee $ Pr(err_{D}(h) \le \varepsilon ) \ge 1 - \delta  $.



\subsection{Finite Hypothesis Space}

Let's start with finite hypothesis space case, $ \left\lvert 
\mathcal{H} \right\rvert < \infty  $.

============================================\\
Theorem:

Suppose hypothesis space $ \mathcal{H} $ is finite, Algorithm $ A $ finds
hypothesis $ h_{A} \in \mathcal{H} $ is consistent with $ m $ random 
training examples where
\begin{equation}
				m \ge \frac{1}{\varepsilon }(\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta })
\end{equation}

Then we can say 
\begin{equation}
				Pr \left[ 
				err_{D}(h_{A}) > \varepsilon \right] 
				\le \delta 
\end{equation}

or it is PAC learnable.


Equivalently, we can write,
\begin{align*}
				&\text{with } prob. \ge 1 - \delta,\\
				&err_{D}(h) \le  \frac{\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta }}{m} = \varepsilon 
\end{align*}
where $ h $ is a random variable.\\

================================\\
Notice, {\underline {consistent}} here is NOT what we mean in Econometrics!!\\
Here, consistent means {\underline {hyp.}} makes NO mistake in training examples.

================================


In last section we have 
\begin{equation*}
				m \ge  \frac{\ln \frac{1}{\delta }}{\varepsilon }.
\end{equation*}

Now, we add $ \ln \left\lvert \mathcal{H} \right\rvert  $ to the RHS,
\begin{equation*}
				m \ge  \frac{1}{\varepsilon }
				(\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta })
\end{equation*}

Term $ \ln \left\lvert \mathcal{H} \right\rvert  $ measures the 
{\underline {complexity}} of the hypothesis space.\\

================================\\
Complexity:\\
If we want to give each hyp. a name in $ \mathcal{H} $, how many bits we need
to do that?

The number of bits you need would be $ log_{2}\left\lvert \mathcal{H}
\right\rvert  $, where $ \left\lvert \mathcal{H} \right\rvert  $ stands for
the {\underline {number of hypothesis}} in this class.
So, here, we use $ \ln \left\lvert \mathcal{H} \right\rvert  $ roughly
measures the complexity of $ \mathcal{H} $.

================================\\

Recall three conditions we need to do machine learning:
\begin{enumerate}
        \item enough data
				\item fit the training set well
				\item use simple classifier
\end{enumerate}

Equation (2) gives us an accurate classifier. Equation (1) tells us we have
enough data.
Remember, Alg. $ A $ finds a $ h_{A} $ is consistent with training data.
And $ \ln \left\lvert \mathcal{H} \right\rvert  $ bound the complexity.
The more complex it is, the higher value would $ \ln \left\lvert 
\mathcal{H}\right\rvert  $ be. Then, $ m $ will also go up. It means with
more complex hypothesis space, we need more data to get an accuracy prediction.
So, {\underline {we need to limit the complexity}}.
Note, $ \left\lvert \mathcal{H} \right\rvert  $ is the cardinality, not 
absolute value.\\


================================\\
Theorem:

Assume given $ m \ge \frac{1}{\varepsilon }(
\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta }) $ examples,
with $ prob. \ge 1 - \delta  $, 

$ \forall h \in \mathcal{H}: $\\
if $ h $ is consistent, then $ err_{D}(h) \le \varepsilon  $.

Note, 

if $ err_{D}(h) \le \varepsilon  $, we say $ h $ is $ \text{ 
$ \varepsilon  $-good} $

if $ err_{D}(h) \le \varepsilon  $, we say $ h $ is $ \text{ 
$ \varepsilon  $-bad} $

================================\\
Proof:

$ Pr(\forall h \in \mathcal{H}: h \text{ consistent } \Rightarrow
h \text{ is } \text{ $ \varepsilon  $-good}) \ge 1 - \delta  $

can be written as 

$ Pr(\exists h \in \mathcal{H}: h \text{ $ cons. $ and $ h $
is $ \varepsilon  $-bad} ) \le \delta  $. 

================================\\
Remember $ h $ is a random variable
because it depends on training samples. But ``$ e $ is $ \varepsilon  $-bad"
is not a RV.

================================

Let's go back to the proof.

Define
\begin{equation*}
				B = \left\{ h \in \mathcal{H}: \text{ $ h $ is $ \varepsilon  $-bad }
				\right\} 
\end{equation*}
So we can write
\begin{align}
				&Pr(\exists h \in \mathcal{H}: \text{ $ h $ cons. } \Rightarrow
				\text{ $ h $ is $ \varepsilon  $-bad })\\
				=& Pr(\exists h \in B: h \text{ is cons. })\\
				=& Pr \left( \bigcup _{h \in B} (h \text{ is cons. })  \right) \\
				=& (h_1 \in B \text{ cons. }) \cup (h_2 \in B \text{ cons. })\cdots
				   (h_{n} \in B \text{ cons. })\\
				\le & \sum\limits_{h \in B} Pr (h \text{ cons. })
				\quad \text{ recall, }
				Pr(a \cup b) \le Pr(a) + Pr(b)
\end{align}

Then what is the $ Pr(h \text{ cons. }) $?

Because $ x_{i} $ are $ i.i.d. $,
\begin{align}
				Pr(h \text{ cons. })
				 &= Pr(h(x_1) = c(x_1)\cap \cdots \cap h(x_{m}) = c(x_{m}))\\
				 &= \prod_{i = 1} ^ m Pr(h(x_{i}) = c(x_{i}))\\
				 & \le  (1 - \varepsilon )^{m}
\end{align}
Note, since $ Pr(h(x_{i}) \ne c(x_{i})) \le \varepsilon  $, so
$ Pr(h(x_{i}) = c(x_{i})) \le 1 - \varepsilon  $.

Hence, from equation (7) and (10), we can write

\begin{align*}
				&Pr \left( 
				\bigcup _{h \in B} (h \text{ cons. }) \right) \\
				& \le \sum\limits_{h \in B} Pr (h \text{ cons. })\\
				& \le \left\lvert B \right\rvert (1 - \varepsilon )^{m} \quad
				\text{ Note, }
				\left\lvert B \right\rvert \le \left\lvert \mathcal{H} \right\rvert \\ 
				& \le \left\lvert \mathcal{H} \right\rvert e^{ - \varepsilon m}\\
				& \le  \delta 
\end{align*}


How do get $ \delta  $ from $ \left\lvert \mathcal{H} \right\rvert 
e^{ - \varepsilon m}$?\\
Take log, we have $ \ln \left\lvert \mathcal{H} \right\rvert  - 
\varepsilon m$.
Given $ m \ge \frac{1}{\varepsilon }(\ln \left\lvert \mathcal{H} \right\rvert ) $
\begin{align*}
				em & \ge \ln \left\lvert \mathcal{H} \right\rvert  + \ln 
				\frac{1}{\delta }\\
				- \varepsilon m & \le - \ln \left\lvert \mathcal{H} \right\rvert 
				 - \ln \frac{1}{\delta } =  - \ln \left\lvert \mathcal{H} \right\rvert 
				  + \ln \frac{1}{\delta }\\
				\ln \left\lvert \mathcal{H} \right\rvert   - \varepsilon m
				& \le \ln \delta \\
				\left\lvert \mathcal{H} \right\rvert e^{ - \varepsilon m}
				& \le \delta  \quad \text{ take exponential }\\
				&Q.E.D.
\end{align*}

\subsection{Infinite hypothesis space}


Suppose we have four examples on the real line. $ h_1 $ and $ h_2 $ give us
the same prediction.
\begin{figure}[ht]
    \centering
    \incfig{same-prediction}
    \caption{same prediction}
    \label{fig:same-prediction}
\end{figure}


The prediction would be different if we do this,

\begin{figure}[ht]
    \centering
    \incfig{different-hyp.}
    \caption{different hyp.}
    \label{fig:different-hyp.}
\end{figure}


For each of the four instances, hypothesis's behavior can be either labeling
instance as + or -. So, for $ m $ instances, the max behavior is $ 2^{m} $.
Here we have $ m + 1 $, 5, hypothesis.

\subsubsection{Growth Function}

A set of examples $ \mathcal S = \left\{ x_1, \cdots, x_{m} \right\}  $ contains
$ m $ labeled instances. For particular hypothesis $ h $,
\begin{equation}
				\Pi _{\mathcal{H}}(\mathcal S) = \left\{ 
				\left\{ h(x_1), \cdots, h(x_{m}) \right\} : h \in \mathcal{H} \right\} 
\end{equation}
where $ \Pi _{\mathcal{H}}(\mathcal S) $ is a set of hypothesis behaviors.

We can look at how bad (maximum number) this set can be.
\begin{equation*}
				\Pi _{H}(m) = \max_{\substack{\left\lvert \mathcal S
				\right\rvert = m \\}}
				\left\lvert \Pi _{\mathcal{H}}(\mathcal S) \right\rvert 
\end{equation*}
These are all cardinality, not absolute value. They measures the size.

The Growth function tells us the maximum number (the worst case) of labeling
behavior a hypothesis space can make given a sample with size $ m $.

In figure 6, we have $ m + 1 $ hypothesis to make it an effective hypothesis
space. And $ \Pi _{\mathcal{H}}(m) $ is called the growth function.


Recall, when $ \left\lvert \mathcal{H} \right\rvert  < \infty  $ , we have
\begin{equation*}
				err_{D}(h) \le \frac{\ln \left\lvert \mathcal{H} \right\rvert 
				 + \ln \frac{1}{\delta }}{m}
\end{equation*}

Now, we relax this finite assumption, and we can use the growth function 
to measure the complexity of the hypothesis space.\\

\noindent\fbox{%
    \parbox{\textwidth}{%
    Theorem:

		Given $ m $ training examples, with $ prob. \ge 1 - \delta  $,
		$ \forall h \in \mathcal{H} $, 

		if $ h $ is consistent, then
		\begin{equation}
						err_{D}(h) \le 
						O \left( \frac{\ln \Pi _{\mathcal{H}}(2m) + \ln \frac{1}{\delta }}
						{m} \right) 
		\end{equation}
    }%
}

If the growth function is polynomial (this is a nice case),
\begin{equation*}
				\Pi _{\mathcal{H}}(m) = O(m^{d})
\end{equation*}
where $ d $ is a constant. We use big O here to hide constant terms.
Hence the error becomes,
\begin{equation}
				err_{D}(h) \le \frac{d \ln m  + \ln \frac{1}{\delta }}{m}
\end{equation}


\noindent\fbox{%
    \parbox{\textwidth}{%
						In the nice case, 
						\begin{equation*}
										 \Pi _{\mathcal{H}}(m) = O(m^{d}),
						\end{equation*}
						it is the finite case.

						In the worst case, the growth function realizes all possible
						behaviors,
						\begin{equation*}
										\Pi _{H}(m) = 2^{m},
						\end{equation*}
						it is an infinite case.

    }%
}\\


For any hypothesis space $ \mathcal{H} $, it will be {\underline {either}} the
{\underline {nice case}}, or {\underline {the worst case}}. There's NO other
cases!




In the nice case, {\underline {learning is possible}} because we can solve
the $ err_{D}(h) $ according to the theorem.\\
In the worst case, {\underline {learning is not possible}}.

{\textbf {NB:}} \\
In $ \Pi _{H}(m) = O(m^{d}) $, $ d $ is called the VC-dimension, where
VC is the short form of Vapnik-Chervnenkis.

Before we introduce VC-dimension, we need to know {\underline {shattering}}
first. \\

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Shattering:}}

		Sample $ \mathcal S $ with size $ m $ is shattered by $ \mathcal{H} $, 
		if all behaviors are possible,
		$ \left\lvert \Pi _{H}(\mathcal S) \right\rvert = 2^{m}  $.

		Equivalently, we say sample $ \mathcal S $ is shattered by $ \mathcal{H} $,
		if $ \mathcal{H} $ can dichotomize all elements in sample $ \mathcal S $,
		i.e., $ \Pi _{H}(\mathcal S) = 2^{m} $.
    }%
}\\


\newpage

Here's an example

\begin{figure}[ht]
    \centering
		\incfig{shattering-example}
    \caption{shattering example}
    \label{fig:shattering-example}
\end{figure}

In a three-point example, sample is not shattered because we {\underline {cannot
}} label them using one threshold. Hence, we can shatter two points,
but we can never shatter three points.\\

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Definition:}}

		The VC-dimension for $ \mathcal{H} $ is the maximum size of a sample,
		which can be shattered by $ \mathcal{H} $.
		\begin{equation*}
						\text{ VC-dim }(\mathcal{H}) = 
						\max_{\substack{\\}} \left\{ m: \Pi _{\mathcal{H}}(m) = 2^{m}
						\right\} 
		\end{equation*}
    }%
}\\

In this case, VC-dim(intervals) = 2 because we can only shatter a two-point
sample.


\newpage

Extension:

Linear threshold function (LTF) in $ {\rm I\!R}^{n}  $ looks like this.
The VC-dim(LTF in $ {\rm I\!R}^{n} $) = $ n + 1 $

\begin{figure}[ht]
    \centering
    \incfig{linear-threshold-function}
    \caption{Linear threshold function}
    \label{fig:linear-threshold-function}
\end{figure}


\newpage

VC-dim(LTF that go through the origin) = $ n $
\begin{figure}[ht]
    \centering
    \incfig{ltf-go-through-the-origin}
    \caption{LTF go through the origin}
    \label{fig:ltf-go-through-the-origin}
\end{figure}



For finite hypothesis space $ \mathcal{H} $, 
\begin{equation*}
				\text{ VC-dim }(\mathcal{H}) \le \ln \left\lvert \mathcal{H}
				\right\rvert 
\end{equation*}

where $ \ln \left\lvert \mathcal{H} \right\rvert  $ measures the complexity
of the hypothesis space.





\newpage

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Summary:}}
		
		Given $ m $ training examples, with $ prob. \ge 1 - \delta  $,
		$ \forall h \in \mathcal{H} $:

		If $ h $ is consistent, then

    \begin{align}
						& err_{D}(h) \le \frac{
						\ln \left\lvert \mathcal{H} \right\rvert  + \ln \frac{1}{\delta }}
						{m}, \quad
						\text{ if }\left\lvert \mathcal{H} \right\rvert < \infty
						\\
						& err_{D}(h) \le 
						O \left( \frac{\ln \Pi _{\mathcal{H}}(2m) + \ln \frac{1}{\delta }}
						{m} \right) , \quad
						\text{ for all } \mathcal{H}\text{ not only finite ones, }
    \end{align}


    }%
}\\


\subsubsection{Sauer's Lemma}

Review on textbook page 277

\noindent\fbox{%
    \parbox{\textwidth}{%
    {\textbf {Sauer's Lemma:}}
		
		Given d = VC-dim $ (\mathcal{H}) $,
		\begin{equation}
						\Pi _{\mathcal{H}}(m) \le \sum\limits_{i = 0} ^d \binom{m}{i}
						\le \left( \frac{em}{d} \right) ^{d}, \quad \text{ 
						if $ m \ge d \ge 1 $}
		\end{equation}
		where we can solve binomial term using
		\begin{equation*}
						\binom{m}{i} = \frac{m!}{i!(m - i)!}
		\end{equation*}
    }%
}\\


Recall, for any $ \mathcal{H} $, either\\
$ \Pi _{H}(m) = 2^{m} \quad \forall  m$, this means $ d =  $ VC-dim is infinite.
The worst case

or

$ \Pi _{\mathcal{H}}(m) = O(m^{d}) $ for some constant $ d $.
This is the nice case. It says $ d =  $VC-dim is finite.


If we plug equation (16) into (15),
\begin{align*}
				err_{D}(h) & \le O \left( 
				\frac{\ln \Pi _{\mathcal{H}} + \ln \frac{1}{\delta }}{m}\right) \\
				& \le O \left( 
				\frac{\ln \left( \frac{em}{d} \right)^{d}  + \ln \frac{1}{\delta } }{m
        }\right) \\
				& \le O \left( 
				\frac{d \ln \frac{m}{d} + d + \ln \frac{1}{\delta }}{m}\right)
\end{align*}

So, now, the VC-dim $ =d $ acts like a measure of complexity.

{\textbf {NB:}} the VC-dim also gives us a lower bound of how many examples
we need for training.

================================\\
Note, VC-dim$ (\mathcal{H} )= d $ means that there exists ($ \exists  $) a
sample with size $ d $ which can be shattered by hypothesis space $ \mathcal{H}$.
Here, we say there exists ($ \exists  $). Hence, it does NOT mean that for all
sample with size $ d $ can be shattered. One is enough.

================================


\subsubsection{Empirical Risk Model (ERM)}

There might be some randomness between today's weather and tomorrow's weather.
So, in stead of write $ y $ as a direct function of $ x $, $ y = f(x) $, 
we say instance $ x $ and label $ y $ are in a {\underline {pair}} 
$ (x, y) $. And this pair is followed a distribution $ D $, $ (x, y) \sim D $.

Hence, the measure of hypothesis $ h(\cdot ) $ becomes
\begin{equation*}
				err_{D}(h) = Pr_{(x,y) \sim D} [h(x) \ne y]
\end{equation*}

Now the problem becomes this:

Given sample $ (x_1,y_1), \cdots, (x_{m},y_{m}) $ where $ (x_{i},y_{i})
\sim D$.\\
We want: $ \min_{\substack{h \in \mathcal{H}\\}} err_{D}(h)  $.

This says we want to minimize the error, $ err_{D}(h) $ over all hypothesis in
$ \mathcal{H} $.


Notice, $ err_{D}(h) $ is the generalized error. Our {\textbf {training error}}
from the machine would be
\begin{equation*}
				\widehat{err_{D}(h)} = \frac{1}{m} \sum\limits_{i = 1} ^m
				1 \left\{ h(x_{i} \ne y_{i}) \right\} 
\end{equation*}
where $ 1 \left\{ h(x_{i} \ne y_{i}) \right\} $ is an indicator function.
We can write it in ECON way:
\begin{equation}
				\widehat{err_{D}(h)} = \frac{1}{m}\sum\limits_{i = 1} ^m
				I, \quad
				I = 
				\begin{cases}
								1 \quad \text{ if } h(x_{i}) \ne y_{i}\\
								0 \quad o.w.
				\end{cases}
\end{equation}

Clearly, $ \frac{1}{m}\sum\limits_{i} I	 $ is the sample mean of the indicator
function. It says in what percent the outcome hypothesis is wrong. Remember
$ I = 1 $ when the outcome $ h(x_{i}) $ is NOT the same as true label $ y_{i} $.

Hence, $ \widehat{err_{D}(h)} < 1 $.

And the hypothesis we get from the training machine would be
\begin{equation}
				\widehat{h} = \argmin_{h \in \mathcal{H}} err_{D}(h)
\end{equation}

This method is called {\underline {empirical risk minimization}}, or ERM for
short. The error term $ \widehat{err_{D}(h)} $ is called {\underline {
empirical risk}}.







\end{document}

