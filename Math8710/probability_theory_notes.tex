
\documentclass[12pt]{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\newtheorem{thm}{Theorem}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{Probability Theory}
\author{SynFerLo}
\date{July. 21, 2021}


\begin{document}
\maketitle
\newpage


\section{Terminology}

\subsection{Random variable}
A Random variable is a {\underline {function}} from a set of outcomes
to the real line, attaching numbers to outcomes.





\subsection{IID}
Independence: The RVs $ (X_1, X_2,... Xn) $ is said to be independent
if the occurance of any one, $ X_{i} $, does not influence and is not
influenced by the occurence of any other RV in the set, $ X_{j}, 
j \ne i$,.


Identical Distribution: The density of RVs	are identical
\begin{equation*}
f(x_1; \theta) = f(x_2;\theta) = \cdots = f(x_{n};\theta)
\end{equation*}







\subsection{Event Space}

{\textbf {Outcomes Set:}} {\textbf {S}}, includes all possible distinct outcomes. 
For example, the outcome set of casting a 
dice can be written as $ S = \{1, 2, 3, 4, 5, 6\} $\\

{\textbf {Event space:}} $ \mathfrak{F} $, is a set whose elements are
the events of interest as well as the related events, those we get by
combining the events of interest using set theoretic operations, e.g., 
$  \overline{A},  \overline{B}, A \cap B, A \cup B, ( \overline{A}_{1}
 \cap  \overline{A}_{2}) $, etc.
$ \mathfrak{F} $ is a {\underline {subset}} of $ \bm{S} $.



\begin{equation*}
\text{ for } A \in \mathfrak{F}, \text{ B } \in \mathfrak{F} \text{ 
and} A \cap B = \emptyset, \text{ then } \mathbb{P}(A \cup B) = 
\mathbb{P}(A) + \mathbb{P}(B)
\end{equation*}

For modeling purpose, we need to broaden the event to include not just
elementary outcomes but also combinations of them.


Two extreme event spaces:

(1)$ \mathfrak{F}_{0} = \left\{ \bm{S}, \emptyset  \right\}  $: the
{\textbf {trivial event space.}}

(2)$ \mathcal{P}(S) = \left\{ A:A \subset \bm{S} \right\}  $, i.e., 
the {\textbf {power set}}: the set of all subsets of $ \bm{S} $.

{\textbf {Notice:}} we cannot always use the power set of $ \bm{S} $
as the appropriate event space, because (1) if $ \bm{S} $ is countable
and has $ N $ elements, $ \mathcal{P}(\bm{S}) $ has $ 2^{N} $ elements, 
it contain too many elements; (2) when the outcomes set is uncountable,
such as
\begin{equation*}
\bm{S} = \left\{ x: 0 \le x \le 1, x \in \mathbb{R} \right\} 
\end{equation*}
the power set includes subsets which {\underline {cannot}} be considered
as events and thus cannot be assigned probabilities.

To circumvent these difficulties, we use a field or {\underline {
$ \sigma $-field}},
which ensures if A and B are events then any other events which arise
when we combine these with set theoretic operations are also elements
of the same event space.


{\textbf {Field}}: A collection $ \mathfrak{F} $ of subsets of 
$ \bm{S} $, is said to be a field if it satisfies the conditions:
\begin{enumerate}
\item $ \bm{S} \in \mathfrak{F} $
\item if $ A \in \mathfrak{F} $ then $  \overline{A} $ also belong to
		$ \mathfrak{F} $
\item if $ A,B \in \mathfrak{F} $, then $ (A \cup B) \in \mathfrak{F} $.
\end{enumerate}
This means that $ \mathfrak{F} $ is non-empty, closed under
complementation, finite unions and finite intersections.



{\textbf {Event:}} is a subset of the outcomes set $ \bm{S} $,
i.e., if $ A \subset \bm{S}, A \text{  is an event. } $

{\textbf {Special Events:}}

1. Sure event:  whatever the outcome, $ \bm{S} $ occurs. $ \bm{S} $ is
always a subset of itself, i.e., $ S \subset S $.

2. Impossible event: $ \emptyset  $

3. Any two events A and B are said to be {\textbf {mutually exclusive}} if
\begin{equation*}
A \cap  B = \emptyset 
\end{equation*}


4. The events $ A_1, A_2,..., A_{m} $ is said to constitute a {\textbf {
partition}}
of $ \bm{S} $ if they are:

(1) mutually exclusive, i.e., $ A_{i} \cap A_{j} = \emptyset, 
\forall j \ne j, i,j = 1,2,...,m$ and 

(2) exhaustive, i.e., $ \bigcup_{i = 1}^{m} A_{i} = \bm{S} $.







\subsection{$ \sigma $-field}
A collection $ \mathfrak{F} $ of subsets of $ \bm{S} $, is said to be 
a $ \sigma $-field if it satisfies the conditions:
\begin{enumerate}
\item $ \bm{S} \in \mathfrak{F} $
\item if $ A \in \mathfrak{F} $, then $  \overline{A} \in \mathfrak{F} $
\item if $ A_{i}\in \mathfrak{F} $ for $ i = 1,2,...n,... $ the
		set $ \cup_{i = 1}^{\infty } A_{i} \in \mathfrak{F} $.
\item from 2 and 3, we can deduce that
		\begin{equation*}
		\cap_{i = 1}^{\infty } A_{i}\in \mathfrak{F}, \text{ since }
		\overline{\cup_{i = 1}^{\infty }A_{i}} = \cap_{i = 1}^{\infty }
		 \overline{A}_{i}
		\end{equation*}
\end{enumerate}

{\textbf {Notice:}} a $ \sigma $-field is non-empty and closed under
countable unions and intersections.


\subsection{Borel $ \sigma $-field}
Borel field, or Borel $ \sigma $-field, is the most important $ \sigma $ 
-field defined on the real line $ \mathbb{R} $, denoted by 
$ \mathcal{B}(\mathbb{R}) $.




\subsection{Cartesian Product}
Define the notion of the Cartesian product of two sets by $ A  \times B $
\begin{equation*}
A  \times  B = \left\{ (x,y): x \in A, y \in B \right\} 
\end{equation*}
It is a set of all ordered pairs $ (x,y) $ where $ x \in A $ and 
$ y \in B $.
On example is the Cartesian coordinates of the plane, where X is the
set of points on the x-axis, Y is the set of points on the y-axis,
and $ X  \times  Y $ is the $ xy $-plane.


\begin{figure}[H]
		\center{\includegraphics[scale =.4 ]  {figures/Cartesian_product.png}}
\end{figure}






\subsection{Probability space $ (S, \mathfrak{F}, \mathbb{P}(.)) $}

A {\textbf {probability space }}is a collection of
\begin{enumerate}
\item Outcomes set $ S $
\item Event space $ \mathfrak{F} $, where $ \mathfrak{F} $ is a 
		$ \sigma $-field of subsets of $ S $.
\item Probability set function $ \mathbb{P}(.) $
\end{enumerate}

Notice: the probability function $ \mathbb{P}(.) $ statisfies axioms
[1]-[3]:

[1]: $ \mathbb{P}(S) = 1 $, for any outcomes set $ S $

[2]: $ \mathbb{P}(A) \ge  0 $, for any event $ A \in \mathbb{F} $

[3]: Countable additivity. For a countable sequence of mutually
exclusive events, i.e., $ A_{i} \in \mathfrak{F}, i = 1,2,...,n, ... $
such that $ A_{i} \cap A_{j} = \emptyset \quad \forall i \ne j, i,j = 1,2
,...,n,...$, then
\begin{equation*}
\mathbb{P}(\bigcup_{i = 1}^{\infty }A_{i}) = \sum\limits_{i = 1} ^\infty 
\mathbb{P}(A_{i})
\end{equation*}






\subsection{Sampling Space}
A sequence of $ n $ trials, denoted by $ \mathcal{G}_{n} = 
\left\{ \mathcal{A}_{1},\mathcal{A}_{2},...,\mathcal{A}_{n} \right\} $
where $ \mathcal{A}_{i} $, represents the $ i $th trial of the 
experiment, associated with the product probability space
$ (\bm{S}_{(n)}, \mathfrak{F}_{(n)}, \mathbb{P}_{(n)}) $, is said to
be a sampling space.
We use $ \mathcal{G}_{n} $ for sampling space.


\subsection{Random Trials}
A sequence of trials $ \mathcal{G}_{n}^{IID} := \left\{ 
		\mathcal{A}_{1}, \mathcal{A}_{2},...,\mathcal{A}_{n}
\right\}  $ which is both {\textbf {independent}} and {\textbf {
identical distributed}}, i.e., 
\begin{equation*}
\mathbb{P}_{(n)}(\mathcal{A}_{1} \cap \mathcal{A}_{2}\cap...\cap 
\mathcal{A}_{k}) = \mathbb{P}(\mathcal{A}_{1})\cdot \mathbb{P}
(\mathcal{A}_{2})\cdots\mathbb{P}(\mathcal{A}_{k}) \text{ 
for each $ k = 2,3,...,n $}
\end{equation*}
is referred to as a sequence of Random trials.


\subsection{Statistical Space}
It is the combination of {\underline {a}} simple product probability space, and a 
sequence of Random trials, 
\begin{equation*}
		[(\bm{S},\mathfrak{F}, \mathbb{P}(.))^{n}, G_{n}^{IID}]
\end{equation*}


The more general formulation of a {\textbf {statistical space:}}
\begin{equation*}
		[(\bm{S}_{(n)}, \mathfrak{F}_{(n)}, \mathbb{P}_{(n)}), 
		\mathcal{G}_{n}^{IID}]
\end{equation*}
where each trial $ \mathcal{A}_{i} $ is associated with a different
probability space $ \left\{ (S_{i}, \mathfrak{F}_{i}, \mathbb{P}_{i}
(.))\right\}  $















\section{Set operation}

\begin{equation*}
		\overline{(A \cup B)} =  \overline{A} \cap  \overline{B}
\end{equation*}
\begin{equation*}
		\overline{( \overline{A} \cap  \overline{B})} =  \overline{A} \cup  
		 \overline{B}
\end{equation*}

{\textbf {Difference:}}
\begin{equation*}
A - B = A \cap \overline{B}:=\left\{ x:x \in A \cap  x \notin B \right\} 
\end{equation*}

{\textbf {Symmetric difference:}}
\begin{equation*}
A \Delta B = (A \cap   \overline{B}) \cup ( \overline{A} \cap  B):=
\left\{ x:x \in A \cup  x \in B \cap  x \notin (A \cap  B) \right\} 
\end{equation*}

\begin{figure}[ht]
    \centering
    \incfig{symmetric-difference}
    \caption{Symmetric difference}
    \label{fig:symmetric-difference}
\end{figure}






\section{Useful Probability results}
{\textbf {Them 1:}}
\begin{equation*}
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - 
\mathbb{P}(A \cap B)
\end{equation*}

{\textbf {Them 2: Continuity property of the probability set function}}

For $ \left\{ A_{n} \right\}_{n = 1}^{\infty } \in \mathfrak{F}  $, if
$ \lim_{n \to \infty} A_{n} = A \in \mathfrak{F} $, then
$ \lim_{n \to \infty}\mathbb{P}(A_{n}) = \mathbb{P}(A) $\\


{\textbf {Non-decreasing sequence:}}
A sequence of events $ \left\{ A_{n} \right\}_{n = 1}^{\infty} $ is
called non-decreasing if
\begin{equation*}
A_1 \subset A_2 \subset \cdots \subset A_{n} \subset ...
\end{equation*}
It has a property:
\begin{equation*}
\lim_{n \to \infty}A_{n} = \bigcup_{n = 1}^{\infty }A_{n}
\end{equation*}


{\textbf {non-increasing sequence:}}
\begin{equation*}
A_1 \supset A_2 \supset \cdots \supset A_{n} ...
\end{equation*}

\begin{equation*}
\lim_{n \to \infty}A_{n} = \bigcap_{n = 1}^{\infty }A_{n}
\end{equation*}


{\textbf {Thm 3:}}
\begin{equation*}
\mathbb{P}(\bigcap_{k = 1}^{n}A_{k}) \ge 1 - 
\sum\limits_{k = 1} ^n \mathbb{P}( \overline{A}_{k})	
\end{equation*}
where $ A_{k} \in \mathfrak{F}, k = 1,2,...,n $


{\textbf {Thm: 4:}}
\begin{equation*}
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\end{equation*}
Also, we know
\begin{equation*}
\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)
\mathbb{P}(A)
\end{equation*}
Hence, 
\begin{equation*}
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = 
\frac{\mathbb{P}(A)\mathbb{P}(B|A)}{\mathbb{P}(B)}, \text{ for
$ \mathbb{P}(B) > 0 $}
\end{equation*}
This is called Bayes' formula.

The conditioning probability can be used to determine whether the
occurance of B alters the probability of occurance of A.
If not, $ \mathbb{P}(A|B) = \mathbb{P}(A) $, we say,
they are independent.


Hence we have, 
\begin{equation*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)
\end{equation*}

{\textbf {Notice:}} independent and mutually exlusive are not the same.
The latter one does not involve probability.
If $ A $ and $ B $ are mutually exlusive, 
\begin{equation*}
\mathbb{P}(A \cap B) = 0, \text{ since } A \cap B = \emptyset 
\end{equation*}
If $ A $ and $ B $ are independent and $ \mathbb{P}(A) > 0 
\mathbb{P}(B) > 0$,
\begin{equation*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\cdot \mathbb{P}(B) > 0
\end{equation*}






\section{cdf and pdf}

{\textbf {Density function:}}

It is used to assign probability to event.
\begin{equation*}
f_{x}(x):= \mathbb{P}(X = x), \quad \forall  x \in \mathbb{R}_{X}
\end{equation*}
For $ x \notin \mathbb{R}_{X}, \quad X^{ - 1}(x) = \emptyset  $ and 
thus $ f_{x}(x) = 0, \quad \forall x \notin \mathbb{R}_{X} $.

{\textbf {Note:}} $ (X = x) $ is a shorthand notation for 
$ A_{x} = \left\{ s:X(s) = x \right\}  $.


\subsubsection{For a discrete RV}
\begin{equation*}
F_{X}(x_{k}) = \mathbb{P}(\left\{ 
		s:X(s) \le x_{k}
\right\} ) = \sum\limits_{i = 1} ^k f_{x}(x_{i}), \quad
for k = 1,2,...,n
\end{equation*}
{\textbf {Properties:}}
\begin{equation*}
f_{x}(x) \ge 0, \quad \forall x \in \mathbb{R}_{X}
\end{equation*}
\begin{equation*}
\sum\limits_{x_{i} \in \mathbb{R}_{X}} f_{x}(x_{i}) = 1
\end{equation*}
\begin{equation*}
		F_{X}(b) - F_{X}(a) = \sum\limits_{a < x_{i} \le b} f_{x}(x_{i}),
		\quad a < b, \quad a,b \in \mathbb{R}
\end{equation*}

Example:\\
Bernoulli RV
\begin{equation*}
f_{x}(1) = \theta \text{ and } f_{x}(0) = (1 - \theta)
\end{equation*}
\begin{equation*}
F_{X}(x) = 
\begin{cases}
0, \quad x < 0,\\
\theta,\quad 0 \le x <1,\\
1,\quad 1 \le x
\end{cases}
\end{equation*}


\subsection{Probability Model}
A probability model is a collection of density functions
\begin{equation*}
\Phi = \left\{ f_{x}(x;\bm{\theta}),\bm{\theta} \in \Theta,
		x \in \mathbb{R}_{X} \right\} 
\end{equation*}




\section{Parameters and Moments}
The most efficient way to deal with the unknown parameters $ \theta $ is
to relate them to the moments of the distribution.


\subsection{Moments}
The {\textbf {moments}} of a distribution are defined in terms of the
mathematical expectation of certain functions of the random variable
$ X $, generically denoted by $ h(X) $
\begin{equation*}
		E[h(X)] = \int_{ - \infty }^{\infty } h(x)\cdot f(x;\bm{\theta})dx 
\end{equation*}
where $ E[h(X)] $ is some function of $ \bm{\theta} $
\begin{equation*}
		E[h(X)] = g(\bm{\theta})
\end{equation*}
By choosing specific forms of the function $ h(X) $, such as:
\begin{equation*}
h(X) = X^{r}, h(X) = \left\lvert X \right\rvert ^{r}, r = 1,2,...,
h(X) = e^{tx}, h(X) = e^{itx},
\end{equation*}
we obtain several functions of the form $ g(\bm{\theta}) $ which involve
what we call {\textbf {moments}} of $ f(x;\bm{\theta}) $.\\

\noindent\fbox{%
\parbox{\textwidth}{%
		{\textbf {Note:}} the best way to handle probability models (
		postulate a statistical model, estimate $ \bm{\theta} $, test
		hypotheses about these parameters, $ \bm{\theta} $, etc.) is often
		via the moments of the postulate probability distribution.

}%
}\\


\subsubsection{Higher Raw moments}
A direct generalization of the {\textbf {mean}} yields the so-called
raw moments. For $ h(X):=X^{r}, r = 2,3,4... $ the raw moments:
\begin{equation*}
\mu'_{r}(\bm{\theta}):=E(X^{r}) = \int_{ - \infty }^{\infty }
x^{r}f_{x}(x;\bm{\theta})dx, \quad r = 1,2,3,...
\end{equation*}

{\textbf {Note:}}

The second raw moment is often useful in deriving the variance.
Recall 
\begin{equation*}
		Var(X) = E(X^{2}) - [E(X)]^{2},
\end{equation*}
where $ E(X^{2}) $ is the second raw moment.


\noindent\fbox{%
\parbox{\textwidth}{%
{\textbf {Lower moments lemma}}

If $ \mu'_{k}:= E(X^{k}) $ exists for some positive integer $ k $, then
all the raw moments of order less than $ k $ also exists, i.e., 
\begin{equation*}
E(X^{i}) < \infty , \quad \forall i = 1,2,...,k - 1
\end{equation*}
}%
}\\


\subsubsection{Moment generating function (MGF)}
A particular convenient way to compute the {\textbf {raw moment}} is
by way of the MGF using the integral with $ h(X) = e^{tx} $
\begin{equation*}
m_{X}(t) := E(e^{tx}) = \int_{ - \infty }^{\infty }e^{tx}f(x)dx,
\text{ for } t \in ( - h,h), h>0
\end{equation*}

Recall, $ e^{\theta} = \sum\limits_{i = 0} ^\infty \frac{\theta^{i}}{i!}$


{\textbf {Moments and MGF:}}

The $ r $th raw moments is the $ r $th derivative of MGF with respect
to $ t $ when $ t = 0 $.
\begin{equation*}
\mu'_{r} = E(X^{r}) = \frac{d^{r}m_{X}(t)}{dt^{r}}\Bigg|_{t = 0}:=
m_{X}^{(r)}(0), r = 1,2,3,...
\end{equation*}

Hence, mean is the raw moment, $ \mu'_{1} = \frac{dm_{X}(t)}{dt}
\Bigg|_{t = 0} = m_{X}(0)$\\

Consider {\textbf {Poisson}} distribution as an example.

\noindent\fbox{%
\parbox{\textwidth}{%
Given the density of Poisson distribution:
\begin{equation*}
f_{x}(x) = \frac{e^{ - \theta}\theta^{x}}{x!}
\end{equation*}
The MGF can be written as
\begin{align*}
m_{X}(t) = E(e^{tX}) &= \sum\limits_{x = 0} ^\infty e^{tx}
\left( \frac{e^{ - \theta}\theta^{x}}{x!} \right) \\
&= e^{ - \theta}\sum\limits_{x = 0} ^\infty \frac{e^{tx}\theta^{x}}{x!}\\
&= e^{ - \theta}\sum\limits_{x = 0} ^\infty \frac{(e^{t}\theta)^{x}}{x!}\\
\end{align*}
Recall
\begin{equation*}
e^{x} = \sum\limits_{i = 0} ^\infty \frac{x^{i}}{i!}	
\end{equation*}
Hence,
\begin{equation*}
m_{X}(t) = e^{ - \theta}e^{e^{t}\theta} = e^{\theta(e^{t} - 1)}
\end{equation*}
}%
}\\

Recall the first moment is the first derivative of the MGF with respect
to $ t $. Hence, we can derive the expectation of Poisson distribution
\begin{align*}
E(X) &= \frac{dm_{X}(t)}{dt}\Bigg|_{t = 0}\\
 &= e^{\theta(e^{t} - 1)}\theta e^{t}\Bigg|_{t = 0}\\
 &= \theta e^{\theta(e^{t} - 1) + t}\Bigg|_{t = 0}\\
 &= \theta
\end{align*}

For the variance, $ Var(X) = E(X^{2}) - (E(X))^{2} $.
\begin{align*}
E(X^{2}) &= \frac{d^{2}m_{X}(t)}{dt^{2}}\Bigg|_{t = 0}\\
 &= \frac{d}{dt}\theta e^{\theta(e^{t} - 1) + t}\Bigg|_{t = 0}\\
 &= \theta e^{\theta(e^{t} - 1) + t}(\theta e^{t} + 1)\Bigg|_{t = 0}\\
 &= \theta(\theta + 1)\\
 &= \theta^{2} + \theta
\end{align*}

Hence, 
\begin{equation*}
Var(X) = \theta^{2} + \theta - \theta^{2} = \theta
\end{equation*}

\noindent\fbox{%
\parbox{\textwidth}{%

{\textbf {Uniqueness lemma:}}

When a MGF exists (it does not always exists), it is unique in the sense
that two RVs X and Y that have the same MGF must have the same
distribution, and conversely.


}%
}\\


\noindent\fbox{%
\parbox{\textwidth}{%
{\textbf {Probability integral transformation lemma:}}

For any continuous RV $ X $, with cdf $ F_{X}(x) $, the RV defined by
$ Y = F_{X}(x) $ has a uniform distribution over the range (0,1), i.e., 
\begin{equation*}
Y = F_{X}(x) \sim \bm{U}(0,1)
\end{equation*}
}%
}\\

{\textbf {Proof:}}

Derive the MGF for uniform distribution first.
Recall the pdf of uniform distribution,
\begin{equation*}
f(x) = \frac{1}{b - a}
\end{equation*}
Then, we can write the MGF as the following
\begin{align*}
m_{X}(t) = E(e^{tx}) &= \int_{a}^{b} e^{tx} \frac{1}{b - a}dx\\
 &= \frac{1}{b - a}\int_{a}^{b} e^{tx}dx\\
 &= \frac{1}{b - a} \left( \frac{e^{tx}}{t}\Bigg|^{b}_{a} \right) \\
 &= \frac{1}{b - a}\left( 
		 \frac{e^{bt} - e^{at}}{t}
 \right) \\
 &= \frac{e^{t} - 1}{t}, \text{ if $ (a,b) $ is $ (0,1) $ }
\end{align*}

Now let's derive the MGF for $ Y = F_{X}(x) $
\begin{align*}
m_{Y}(t) = E(e^{tY}) &= \int_{ - \infty }^{\infty }e^{tF(x)} f(x)dx\\
 &= \int_{ - \infty }^{\infty }e^{tF(x)}dF\\
 &= \frac{e^{tF(x)}}{t}\Bigg|^{\infty }_{ - \infty }\\
 &= \frac{e^{t} - 1}{t}
\end{align*}
Note: $ F(\infty ) = 1 $, $ F( - \infty ) = 0 $.





\subsubsection{Cumulants}
A cumulant generating function is the logarithm of a MGF.
\begin{equation*}
\psi_{X}(t) = \ln (m_{X}(t)) = \sum\limits_{r = 1} ^\infty 
\bm{\kappa}_{r}\frac{t^{r}}{r!}, \text{ for } t \in ( - h,h), h >0
\end{equation*}
where $ \bm{\kappa}_{r}, r = 1,2,3,... $ are referred to as cumulants(or 
semi-invariants).
\begin{equation*}
\bm{\kappa}_{1} =E(X) = \frac{d \psi_{X}(t)}{dt}\Bigg|_{t = 0}
\end{equation*}
\begin{equation*}
\bm{\kappa}_{2} = Var(X) = \frac{d^{2} \psi_{X}(t)}{d t^{2}}
\Bigg|_{t = 0}
\end{equation*}

The relation between the first few cumulants and the raw moments are
as follows:
\begin{align*}
\bm{\kappa}_{1}  &= \mu'_{1},\\
\bm{\kappa}_{2} &= \mu'_{2} - (\mu_{1}')^{2},\\
\bm{\kappa}_{3}  &= \mu'_{3} - 3 \mu'_{2}\mu'_{1} + 2(\mu'_{1})^{3},\\
\end{align*}


{\textbf {Properties:}}
The cumulants are often {\textbf {preferable}} to the moments for 
several reasons including:

1. In the case of the Normal distribution: $ \bm{\kappa}_{r} = 0,
r = 3,4,...$

2. The $ r $th cumulant is $ r $th-order homogeneous:
$ \bm{\kappa}_{r}(\alpha X) = \alpha^{r}\bm{\kappa}_{r}(X), r = 1,2,.. $

3. The $ r $th cumulant is a function of the moments of order up to 
$ r $.

4. For {\textbf {independent}} RVs, the cumulant of the sum is the sum
of the cumulants:
\begin{equation*}
\bm{\kappa}_{r}(\sum\limits_{k = 1} ^n X_{k}	) = \sum\limits_{k = 1} ^n
\bm{\kappa}_{r}(X_{k}), r = 1,2,...
\end{equation*}

\noindent\fbox{%
\parbox{\textwidth}{%
Recall the definition of homogeneous function:
\begin{equation*}
f(rx,ry) = r^{k}f(x,y)
\end{equation*}
is called function $ f(.) $ is homogeneous of degree $ k $.
}%
}\\


\subsubsection{Characteristic function(CF)}
The existence of the MGF depends on $ m_{X}(t) $ being finite on the
interval $ ( - h,h) $. In such a case, all the moments $ E(X^{r}) $
are {\textbf {finite.}} If $ E(X^{r}) $ is {\textbf {not finite}} for
some $ r $, $ m_{X}(t) $ is not finite on the interval. To solve this
we define the {\textbf {characteristic function}} (Cramer, 1946):
\begin{equation*}
\phi_{X}(t):=E(e^{itX}) = \int_{ - \infty }^{\infty }  e^{itX}f(x)dx
=m_{X}(it), \text{ for } i = \sqrt { - 1},
\end{equation*}
CF always exist since for all $ t	 $, $ \phi_{X}(t) $ is bounded:
\begin{equation*}
\left\lvert \phi_{X}(t) \right\rvert \le E(\left\lvert 
e^{itX}\right\rvert )
\end{equation*}

Hence, in many cases, we can derive the CF using MGF.
The CF is related to the moments (when they exist) via
\begin{equation*}
\phi_{X}(t) = \sum\limits_{r = 0} ^	\infty  \frac{(it)^{r}}{r!}
\mu_{r}', \text{ for } t \in ( - h,h), h > 0
\end{equation*}


\subsection{Problem of moments}
The existence and the uniqueness of the moment. In general, the answer
is no. However, under certain conditions, the answer is yes.

\subsubsection{Lemma 1 (existence)}
A sufficient (not certainly necessary) condition for the existence of
moments is that the {\textbf {support}} of the RV $ X $ is a {\textbf {
bounded	interval}}, i.e., $ \mathbb{R}_{X}:=[a,b] $, where
$  - \infty < a <b <\infty  $. In this case, {\textbf {all moments
exist:}}
\begin{equation*}
\mu'_{k} = \int_{a}^{b} x^{r}f(x)dx < \infty , \forall k = 1,2,...
\end{equation*}


\subsubsection{Lemma 2 (uniqueness)}
The moments $ \left\{ \mu'_{k}, k = 1,2,... \right\}  $(assuming they
exist) determine the distribution function {\textbf {uniquely}} if:
\begin{equation*}
\lim_{n \to \infty}\left( \sup_{\substack{  }}
		\left[ 
				(2n)^{ - 1}(\mu'_{2n})^{\frac{1}{2n}}
		\right] 
\right) < \infty 
\end{equation*}

Note: sup for supremum (the least upper bound), and inf for infimum
(the largest lower bound).

\begin{figure}[H]
		\center{\includegraphics[scale =.3 ]  {figures/supremum.png}}
		\includegraphics[scale = .3]{figures/infimum.png}
\end{figure}


\subsection{Higher Central Moments}
The notion of the {\textbf {variance}} can be extended to define the
{\textbf {central moments}} using the sequence of functions 
$ h(X):=(X - E(X))^{r}, r = 3,4,... $
\begin{equation*}
\mu_{r}(\bm{\theta}):=\int_{ - \infty }^{\infty }
(x - \mu)^{r}f(x;\bm{\theta})dx, r = 2,3,...	
\end{equation*}


We normally derive the central moments by using the relationship
with the raw moments and the cumulants.
\begin{align*}
\mu_{2} &= \mu'_{2} - (\mu'_{1})^{2} & \kappa_2=\mu_2\\
\mu_3 &= \mu'_{3} - 3 \mu'_{2}\mu'_{1} + 2(\mu'_{1})^{3} & \kappa_3=
\mu_3\\
\mu_4 &= \mu'_{4} - 4 \mu'_{3}\mu'_{1} + 6 \mu'_{2}(\mu'_{1})^{2} - 
3(\mu'_{1})^{4} & \kappa_4=\mu_4 - 3 \mu_2^{2}
\end{align*}



\subsubsection{Symmetry}
A RV $ X $ with density $ f(x) $ is said to be symmetric about a 
point a if
\begin{equation*}
f(a - x) = f(a + x), \forall x \in \mathbb{R}_{X}
\end{equation*}
\begin{equation*}
F_{X}(a - x) + F_{X}(a + x) = 1, \forall x \in \mathbb{R}_{X}
\end{equation*}


\subsubsection{Skewness}
It gives us some ideas about the shape/possible asymmetry of a 
density function around the mean is the {\textbf {skewness}} coefficient
.
\begin{equation*}
{\textbf {Skewness:}} \alpha_3(X)=\frac{\mu_3}{(\sqrt {\mu_2})^{3}}
\end{equation*}
Note, $ \sqrt {\mu_2} = (Var(X))^{\frac{1}{2}} = SD $.

If the distribution is symmetric around the mean, $ \alpha_3 = 3 $,
{\textbf {the converse does not hold!}}


\subsubsection{Kurtosis}
Kurtosis measures the peakedness of a density in relation to the
shape of the tail.
\begin{equation*}
{\textbf {Kurtosis:}}\alpha_4(X) = \frac{\mu_4}{(\mu_2)^{2}}
\end{equation*}


In the case of Normal distribution $ \alpha_4 = 3 $, and it is referred
to as a mesokurtic distribution. If a distribution is flatter than
this, we call it platykurtic. And if it is steeper than this, we call
it leptokurtic



\subsubsection{Quantile function}
$ F(x_{p}) = p $
The value $ p $ is know as the $ p $th {\textbf {percentile}}, and 
the value $ x_{p} $ the corresponding quantile.
\begin{equation*}
x_{\frac{1}{4}} = F^{ - }(0.25),\quad x_{\frac{3}{4}} = F^{ - }(0.75)
\end{equation*}
where $ F^{ - }(.) $ is known as the {\textbf {quantile function}}.

\begin{equation*}
F_{X}^{ - }(.):(0,1) \rightarrow \mathbb{R}_{X}
\end{equation*}

For example, In the case of standard Normal distribution,
\begin{equation*}
x_{\frac{1}{4}} = -0.6745, \quad x_{\frac{3}{4}} = 0.6745
\end{equation*}

For an arbitrary Normal distribution,
\begin{equation*}
x_{\frac{1}{4}} = \mu - 0.6745 \sigma, \quad x_{\frac{3}{4}} = 
\mu  + 0.6745 \sigma
\end{equation*}


\subsubsection{Interquartile range}
$ IQR(X): = (x_{\frac{3}{4}} - x_{\frac{1}{4}}) $







\subsection{Mean}

For $ h(X):=X $, where $ X $ takes values in $ \mathbb{R}_{X} $, we can
write the {\textbf {mean}} of the distribution
\begin{equation*}
E(X) = \int_{ - \infty }^{\infty }x \cdot  f_{x}(x;\bm{\theta})dx \quad
\text{ for continuous variables, }
\end{equation*}

\begin{equation*}
E(X) = \sum\limits_{x_{i} \in \mathbb{R}_{X}} x_{i} \cdot f_{x}(x_{i};
\bm{\theta}) \quad \text{ for discrete random variables. }
\end{equation*}

\subsubsection{Properties}
1. $ E(c) = c $

2. $ E(aX_1 + bX_2) = aE(X_1) + bE(X_2) $


\subsection{Variance}
For $ h(X) := [X - E(X)]^{2} $ the integral yields the variance:
\begin{equation*}
		Var(X):= E[(X - E(X))^{2}] = \int_{ - \infty }^{\infty }[
		x - \mu]^{2}f_{x}(x;\bm{\theta})dx 
\end{equation*}
In the case of {\textbf {discrete}} RVs the integral is replaced by the
{\textbf {summation}}.

\begin{equation*}
Var(X):= \sum\limits_{i = 1}^{n} (X_{i} - \mu)^{2}f_{x}(x;\bm{\theta})
\end{equation*}

\subsubsection{Properties}
1. $ Var(c) = 0 $

2. $ Var(aX_1 + bX_2) = a^{2}Var(X_1) + b^{2}Var(X_2) + 2abCov(X_1,X_2)
$

\noindent\fbox{%
\parbox{\textwidth}{%
{\textbf {Bienayme's lemma}}
If $ X_1,X_2,...,X_{n} $ are Independent Distributed RVs:
\begin{equation*}
Var \left( \sum\limits_{i = 1} ^n a_{i}X_{i}	 \right) =
\sum\limits_{i = 1} ^n a^{2}_{i}Var(X_{i})	
\end{equation*}
}%
}\\


\subsection{Standard Deviation}
Std. Deviation is the square root of the variance.

\begin{equation*}
		SD(X) = [Var(X)]^{\frac{1}{2}} = \sigma
\end{equation*}

{\textbf {Note:}}
When we need to {\underline {render a RV free of its units of
measurement}}, we divide it by its SD, i.e., we define the standardized
variable:
\begin{equation*}
		X^{*}:=\frac{X}{[Var(X)]^{\frac{1}{2}}}, \text{ where } 
		Var(X^{*}) = 1
\end{equation*}





\subsection{Chebyshev's inequality}
Let X be a RV with bounded variance:
\begin{equation*}
\mathbb{P}(|X - E(X)| > \varepsilon) \le \frac{Var(X)}{\varepsilon^{2}},
\text{ for any }\varepsilon > 0
\end{equation*}





\subsection{General Chebyshev's Inequality}
Let $ X(.): S \rightarrow \mathbb{R}_{X}:=(0,\infty ) $ be a positive
RV, and let $ g(.):(0,\infty ) \rightarrow (0,\infty ) $ be a positive
and increasing function. Then for each $ \varepsilon >0 $:
\begin{equation*}
\mathbb{P}(g(X) \ge  \varepsilon) \le  \frac{E(g(X))}{g(\varepsilon)}
\end{equation*}


\subsection{Markov's inequality}
Let $ X $ be a RV such that $ E(|X|^{p}) < \infty  $ for $ p > 0 $:
\begin{equation*}
\mathbb{P}(|X| \ge  \varepsilon) \le \frac{E(|X|^{p})}{\varepsilon^{p}}
\end{equation*}


\subsection{Jensen's inequality}
Let $ \varphi(.): \mathbb{R} \rightarrow \mathbb{R} $ be a convex 
function, i.e.,:
\begin{equation*}
\lambda \varphi(x) + (1 - \lambda)\varphi(y) \ge \varphi(\lambda x  + (
1 - \lambda)y), \quad \lambda \in (0,1), \quad, x,y \in \mathbb{R}.
\end{equation*}
Assuming that $ E(\left\lvert X \right\rvert ) < \infty  $, then
\begin{equation*}
\varphi(E(X)) \le  E(\varphi(X))
\end{equation*}


\subsection{Minkowski's inequality}
Let $ X $ and $ Y $ be RVs such that $ E(|X|^{p}) < \infty  $, and
$ E(|Y|^{p}) < \infty  $, where $ 1 \le p < \infty  $ then:
\begin{equation*}
		E(|X + Y|^{p})^{\frac{1}{p}} \le  E(|X|^{p})^{\frac{1}{p}} + 
		E(|Y|^{p})^{\frac{1}{p}}
\end{equation*}







\section{Sampling Model}

\subsection{Joint Distribution}
For Discrete RVs:

The {\textbf {joint density}} function is defined by:
\begin{align*}
		&f(.,.): \mathbb{R}_{X} \times \mathbb{R}_{Y} \rightarrow [0,1]\\
		&f(x,y) = \mathbb{P}\left\{ 
				s:X(s) = x	, Y(s) = y	\right\} , (x,y) \in \mathbb{R}_{X}
				 \times \mathbb{R}_{Y}
\end{align*}


For Continuous RVs:

The joint distribution function:
\begin{equation*}
		F_{XY}(.,.): \mathbb{R}^{2} \rightarrow [0,1],
\end{equation*}
\begin{equation*}
F_{XY}(x,y) = \mathbb{P}\left\{ 
s:X(s) \le x, Y(s) \le y \right\} = P_{XY}(( - \infty ,x)  \times 
( - \infty , y)), \quad(x,y) \in \mathbb{R}^{2}
\end{equation*}



The joint cdf can also be defined on intervals of the form $ (a,b] $
\begin{equation*}
\mathbb{P}\left\{ s:x_1 < X(s) \le x_2, y_1 < Y(s) \le y_2 \right\} =
F(x_2,y_2) - F(x_1,y_2) - F(x_2,y_1) + F(x_1,y_1)
\end{equation*}

\subsubsection{joint density}
{\textbf {Bivariate}}

Assuming that $ f(x,y) > 0 $ exists, the joint density function is 
defined via:
\begin{equation*}
F(x,y) = \int_{ - \infty }^{x} \int_{ - \infty }^{y}
f(u,v) du dv
\end{equation*}

In the case where $ F(x,y) $ is differentiable at $ (x,y) $ we can 
derive the {\textbf {joint density}} by partial differentiation:
\begin{equation*}
f(x,y) = \frac{\partial^{2} F(x,y)  }{\partial x \partial y },
\end{equation*}
at all continuity points of $ f(x,y) $.


{\textbf {Properties}}

1. $ f(x,y) \ge 0 \forall  (x,y) \in \mathbb{R}_{X} \times \mathbb{R}
_{Y}$

2. $ \int_{ - \infty }^{\infty }\int_{ - \infty }^{\infty }f(x,y)
dx \cdot  dy = 1$

3. $ F_{XY}(a,b) = \int_{ - \infty }^{a} \int_{ - \infty }^{b}
f(x,y)dx dy$

4. $ f(x,y) = \frac{\partial ^{2} F(x,y) }{\partial x \partial y } $

5. $ \sum\limits_{i = 1} ^\infty \sum\limits_{j = 1} ^\infty f(x
_{i},y_{j}) = 1		, 
F(x_{k},y_{m}) = \sum\limits_{i = 1} ^k \sum\limits_{j = 1} ^	m
f(x_{i}, y_{j})
$\\


{\textbf {n-random variables:}}

1. $ f(x_1,x_2,...,x_{n}) \ge 0, \forall (x_1,x_2,...,x_{n}) \in 
\mathbb{R}_{X}^{n}$

2. $ \int_{ - \infty }^{x_1}\int_{ - \infty }	^{x_2} \cdots
\int_{ - \infty }^{x_{n}} f(x_1,x_2,...,x_{n}) dx_1dx_2 \cdots dx_{n}
=1$

3. $ F(x_1,x_2,...,x_{n}) = \int_{ - \infty }^{x_1}\int_{ - \infty }^{
x_2} \cdots \int_{ - \infty }^{x_{n}} f(u_1,u_2,...,u_{n})du_1 du_2 
... du_{n}$





\subsection{Joint moments}
We define the {\textbf {joint product moments}} of order $ (k,m) $ by:
\begin{equation*}
\mu'_{km} = E \left\{ X^{k}Y^{m} \right\} , k,m = 0,1,2,...,
\end{equation*}
and the {\textbf {joint central moments}} of order $ (k,m) $ are defined
by:
\begin{equation*}
\mu_{km} = E \left\{ 
		(X - E(X))^{k}(Y - E(Y))^{m}
\right\} , k,m = 0,1,2,...
\end{equation*}

\begin{align*}
&\mu'_{10} = E(X), &\mu_{10} &= 0\\
&\mu'_{01} = E(Y), &\mu_{01} &= 0\\
&\mu'_{20} = E(X)^{2} + Var(X), &\mu_{20} &= Var(X)\\
&\mu'_{02} = E(Y)^{2} + Var(Y), &\mu_{02} &= Var(Y)\\
&\mu'_{11} = E(XY), &\mu_{11} &= E[(X - E(X))(Y - E(Y))]
\end{align*}

The most widely used joint moment is the {\textbf {covariance}}, 
defined by
\begin{equation*}
\mu_{11}:= Cov(X,Y) = E \left\{ 
		[X - E(X)][Y - E(X)]
\right\} 
\end{equation*}

{\textbf {Properties:}}

1. $ Cov(X,Y) = E(XY) - E(X)\cdot E(Y) $

2. $ Cov(X,Y) = Cov(Y,X) $

3. $ Cov(aX + bY, Z) = a Cov(X,Y) + b Cov(Y,Z) $, for $ (a,b) \in
\mathbb{R}^{2}$







\subsection{Marginal Distribution}
The {\textbf {marginal distribution}} is derived via a limiting
process of the join cdf:
\begin{equation*}
F_{X}(x) = \lim_{y \to \infty}F(x,y) \text{ and } F_{Y}(y) = 
\lim_{x \to \infty}F(x,y)
\end{equation*}

For example, consider the bivariate exponential distribution:
\begin{equation*}
F(x,y) = (1 - e^{ - \alpha x})(1 - e^{ - \beta y}), \alpha,\beta,x,y >0
\end{equation*}
The marginal distribution:
\begin{align*}
F_{X}(x) = \lim_{y \to \infty}F(x,y) &= \lim_{y \to \infty}
(1 - e^{ - \alpha x})(1 - e^{ - \beta y})\\
 &= 1 - e^{ - \alpha x}\\
F_{Y}(y) = \lim_{x \to \infty}F(x,y)  &= \lim_{x \to \infty}  
(1 - e^{ - \alpha x})(1 - e^{ - \beta y})\\
 &= 1 - e^{ - \beta y}
\end{align*}

\subsection{Defined with density funciton}
\subsubsection{Continuous RV}

\begin{equation*}
F_{X}(x) = \lim_{y \to \infty} F(x,y) = \lim_{ y\to \infty}
\int_{ - \infty }^{x} \int_{ - \infty }^{y}f(x,y)dydx = 
\int_{ - \infty }^{x}\left[ 
		\int_{ - \infty }^{\infty }f(x,y)dy 
\right]  dx
\end{equation*}
And,
\begin{equation*}
f_{x}(x) = \int_{ - \infty }^{\infty }f(x,y)dy, x \in \mathbb{R}_{X} 
\end{equation*}
\begin{equation*}
f_{y}(y) = \int_{ - \infty }^{\infty }f(x,y)dx, y \in \mathbb{R}_{Y} 
\end{equation*}



\subsubsection{Discrete RV}

\begin{equation*}
f_{x}(x) = \sum\limits_{i = 1} ^\infty f(x,y_{i}), X \in \mathbb{R}_{X}	
\end{equation*}
\begin{equation*}
f_{y}(y) = \sum\limits_{i = 1} ^\infty f(x_{i},y), y \in \mathbb{R}_{y}	
\end{equation*}


\section{Conditional Distribution}
\subsection{Discrete RVs}
Two RVs $ X $ and $ Y $
\begin{equation*}
A = \left\{ Y = y \right\} , B = \left\{ X = x \right\} 
\end{equation*}

\begin{align*}
& \mathbb{P}(X = x) = f(x)\\
& \mathbb{P}(Y = y,X = x) = f(x,y)\\
& \mathbb{P}(Y = y|X = x) = f(y|x)
\end{align*}

\subsection{Continuous RVs}

Two RVs $ X $ and $ Y $
\begin{equation*}
A = \left\{ X \le x \right\} , B = \left\{ Y \le  y \right\} 
\end{equation*}

The conditional cdf:
\begin{equation*}
F_{Y|X}(y|X = x) = \lim_{h \to 0^{ + }}
\frac{\mathbb{P}(Y \le y, x \le X \le x + h)}{\mathbb{P}(
x \le X \le x + h)}
= \int_{ - \infty }^{y }  \frac{f(x,u)}{f_{x}(x)}du
\end{equation*}

{\textbf {Note:}}

This suggest that we could indeed define the conditional density 
function but we {\textbf {should not}} interpret it as {\textbf {
assigning}} probabilities because:
\begin{equation*}
		f(.|x): \mathbb{R}_{Y}\rightarrow[0,\infty )
\end{equation*}

\subsubsection{Properties:}
1. $ f(y|x) \ge 0, \quad \forall  y \in \mathbb{R}_{Y} $

2. $ \int_{ - \infty }^{\infty }f(y|x)dy = 1  $

3. $ F(y|x) = \int_{ - \infty }^{y}f(u|x)du  $

For example:

Consider the case where the joint density function:
\begin{equation*}
f(x,y) = 8xy, \quad 0 < x < y, 0 < y < 1
\end{equation*}
The marginal densities of $ x $ and $ y $ can be derived from the
joint density:
\begin{align*}
f_{x}(x) &= \int_{x}^{1} (8xy)dy = 4xy^{2}|^{y = 1}_{y = x}=
4x(1 - x^{2}), \quad 0 < x < 1\\
f_{y}(y) &= \int_{0}^{y}(8xy)dx = 4x^{2}y|_{x = 0}^{x = y} = 4y^{3} ,
\quad 0 < y < 1
\end{align*}

Then, we can derive the conditional densities:
\begin{align*}
f(y|x) &= \frac{8xy}{4x(1 - x^{2})} = \frac{2y}{(1 - x^{2})}, \quad
x < y < 1, 0 < x < 1\\
f(x|y) &= \frac{8xy}{4y^{3}} = \frac{2x}{y^{2}}, \quad
0 < x< y, 0 < y < 1
\end{align*}


{\textbf {Note:}}

The range of $ X $ and $ Y $ is constrained by each other.



\subsection{Continuous and Discrete RVs}
It turns out that a most convenient way to specify such a joint
distribution is via the conditional density

Consider the case where $ F(x,y) $ is the joint cdf of the RV 
$ (X,Y) $ where $ X $ is {\textbf {discrete}} and $ Y $ is {\textbf {
continuous}}. Let $ \mathbb{R}_{X} = \left\{ x_1,x_2,... \right\}  $
be the range of values of the RV $ X $. The joint cdf is completely
determined by the sequence of pairs of a marginal probability and the
associated conditional density:
\begin{equation*}
(f_{x}(x_{k}),f(y|x_{k}), \forall y_{k} \in \mathbb{R}_{X})
\end{equation*}

The only difficulty in this result is how to specify the  conditional
density. It is defined by:
\begin{equation*}
		f(y|x_{k}) = \frac{1}{f_{x}(x_{k})}\frac{d[F(x_{k},y) - 
				F(x_{k} - 0,y)]}{dy},
\end{equation*}
where the notation $ (x_{k} - 0) $ indicates taking the derivative from
the left, such that:
\begin{equation*}
F(x,y) = \sum\limits_{x_{k} \le x} f_{x}(x_{k}) \int_{ - \infty }^{y} 
f(u|x_{k})du
\end{equation*}
Similarly, the marginal distribution of the random variable $ Y $ is 
defined by:
\begin{equation*}
F_{Y}(y) = \sum\limits_{x_{k} \in \mathbb{R}_{X}} f_{x}(x_{k})
\int_{ - \infty }^{y} f(u|x_{k})du
\end{equation*}


\subsection{Conditional moments}
\subsubsection{Continuous RVs}
Raw conditional moments:
\begin{equation*}
		E(Y^{r}|X = x) = \int_{ - \infty }^{\infty }y^{r}f(y|x)dy, 
		r = 1,2,...
\end{equation*}
Central moments:
\begin{equation*}
		E \left\{ (Y - E[Y|X = x])^{r}|X = x \right\} = \int_{ - \infty }^{
		\infty } [y - E(y|x)]^{r}f(y|x)dy, r = 2,3,...
\end{equation*}

\subsubsection{Discrete RVs}
Conditional mean:
\begin{equation*}
E(Y|X = x) = \sum\limits_{y \in \mathbb{R}_{Y}} y \cdot f(y|x)
\end{equation*}

Conditional Variance:
\begin{equation*}
		Var(Y|X = x) = \sum\limits_{y \in \mathbb{R}_{Y}} [y - E(y|x)]^{2}
		\cdot f(y|x)
\end{equation*}




\section{Truncation}
In addition to conditioning on events of the form $ \left\{ 
X = x\right\}  $, it is often of interest to condition on events such
as:
\begin{equation*}
\left\{ X > a \right\} , \left\{ X < b \right\} , \text{ or }
\left\{ a < X \le b \right\} 
\end{equation*}
Consider a general case of conditioning on the event 
$ \left\{ a < x \le b \right\}  $, referred to as {\textbf {double
truncation}}. In the case of a {\textbf {discrete}} RV $ X $ with a 
range of values $ \mathbb{R}_{X}:= \left\{ x_1,x_2,... \right\}  $,
the conditional probability function of $ X $ given $ \left\{ 
a < x \le b\right\}  $ should be given by:
\begin{equation*}
f(x_{i}|a < X \le b) = \frac{f(x)}{\sum\limits_{a < x_{j} \le b} 
f(x_{j})}, \text{ for } a < x_{i} \le b
\end{equation*}

In the case of {\textbf {continuous RV}} $ X $ the conditional 
probability would be
\begin{equation*}
f(x|a < x \le b) = \frac{f(x)}{\int_{a}^{b}f(x)dx } = 
\frac{f(x)}{F(b) - F(a)}, \text{ for } a < x \le b
\end{equation*}


The function $ f(x|a < x \le b), f(x|X > a), f(x|X < b) $ are often
referred to as {\textbf {truncated density functions}} and they are 
enjoy the usual properties:

1. $ f(x|a < x \le b) \ge 0, \forall x \in \mathbb{R}_{X} $

2. $ \int_{a}^{b}f(x|a < x \le b)dx = 1  $\\

For example

\begin{equation*}
f(x) = \theta e^{ - \theta x}, \text{ and } F(x) = 1 - e^{ - \theta x},
y > 0
\end{equation*}
The truncation function for $ X > t $ would be

\begin{align*}
f(x|X > t) &= \frac{f(x)}{F(\infty ) - F(t)}\\
 &= \frac{\theta e^{ - \theta x}}{1 - (1 - e^{ - \theta x})}\\
 &= \frac{\theta e^{ - \theta x}}{e^{ - \theta x}}\\
 &= \theta e^{\theta (t - x)}
\end{align*}




\section{Hazard function}

Truncation is a function of $ x $  and $ t $. Viewing ti as a function
of $ t $ only, we define the {\textbf {hazard function:}}
\begin{equation*}
h(t) = \frac{f(t)}{1 - F(t)} = \frac{f(t)}{S(t)}, \quad x >t,
\end{equation*}
where the $ S(t) $ is called the {\textbf {survival function.}}
It can measure the probability of a patient that will survive after
time $ t $, i.e., $ x > t $.

The hazard function  is also called the {\textbf {force of mortality}},
{\textbf {the
instantaneous	failure rate}}, etc. The most common use of the function
is to model a participant's chance of death as a function of their age.





\section{Independence}
Two events $ A $ and $ B $ belongs to event space $ \mathfrak{F} $
are said to be independent if:
\begin{equation*}
\mathbb{P}(A \cap  B)  = \mathbb{P}(A)\cdot \mathbb{P}(B)
\end{equation*}
translating $ A:= (s:X(s) \le x) $, and $ B:= (s:Y(s) \le  y) , s \in S$
, the above condition becomes:
\begin{align*}
& \mathbb{P}(X \le x, Y \le  y) = \mathbb{P}(X \le x)\cdot \mathbb{P}
(Y \le  y) , \text{  for each } (x,y) \in \mathbb{R}^{2}\\
& F_{XY}(x,y) = F_{X}(x)\cdot F_{Y}(y) \text{ for each }(x,y) 
\in \mathbb{R}^{2}\\
& f(x,y) = f_{x}(x)\cdot f_{y}(y), \text{  for each }(x,y) \in 
\mathbb{R}^{2}
\end{align*}

{\textbf {Note:}}

If $ X \text{ and } Y $ are independent:
\begin{equation*}
f(y|x) = f_{y}(y) \quad \forall  y \in \mathbb{R}_{Y}
\end{equation*}


{\textbf {Independence for n RVs}}
\begin{equation*}
F(x_1,x_2,x_3,...,x_{n}) = F_1(x_1)\cdot F_2(x_2)\cdots F_{n}(x_{n}),
\quad \forall  (x_1,...x_{n}) \in \mathbb{R}^{n}
\end{equation*}


\section{Identical distributed}
Two RVs $ X \text{ and } Y $ are said to be Identical Distributed if
$ f_{x}(x; \bm{\theta}_{1}) $ and $ f_{y}(y; \bm{\theta}_{2}) $ are the
same density functions,
\begin{equation*}
f_{x}(x; \bm{\theta}_{1}) \equiv f_{y}(y; \bm{\theta}_{2}), \quad
\forall  (x,y) \in \mathbb{R}_{X}  \times \mathbb{R}_{Y},
\end{equation*}
and
\begin{equation*}
f_{x}(.) = f_{y}(.), \quad \bm{\theta}_{1} = \bm{\theta}_{2} = 
\bm{\theta}
\end{equation*}



\subsection{Random sample}
The sample $ \bm{X}^{IID}_{(n)}:=(X_1,X_2,...,X_{n}) $ is called a 
{\textbf {random sample}} if the RVs are:\\
1. Independent:
\begin{equation*}
f(x_1,x_2,..,x_{n};\bm{\phi}) = \prod_{k = 1} ^ n	f_{k}(k_{k};
\bm{\theta}_{k}) \quad \forall  (x_1,...,x_{n}) \in \mathbb{R}^{n}
\end{equation*}

2. Identical distributed:
\begin{equation*}
f_{k}(x_{k};\bm{\theta}_{k}) = f(x_{k};\bm{\theta}), \quad \forall 
k = 1,2,...,n
\end{equation*}

Putting two together, the joint density for $ \bm{X}^{IID}_{(n)}:=
(X_1,X_2,...,X_{n})$ takes the form:
\begin{equation*}
f(x_1,x_2,...,x_{n}; \bm{\phi}) = \prod_{k = 1} ^n f_{k}(x_{k};
\bm{\theta}_{k}) = \prod_{k = 1} ^ n f(x_{k};\bm{\theta}), \quad
foral (x_1,...,x_{n}) \in \mathbb{R}^{n}
\end{equation*}

Note that $ f_{k}(x_{k};\bm{\theta}_{k}) $ denotes the {\textbf {
marginal distribution}} of $ X_{k}(.) $, derived by integrating out
all the other RVs apart from $ X_{k}(.) $, i.e., 
\begin{equation*}
f_{k}(x_{k};\bm{\theta}_{k}) = \int_{ - \infty }^{\infty } 
\int_{ - \infty }^{\infty } \cdots \int_{ - \infty }^{\infty }f(
x_1,...x_{k - 1}, x_{k}, x_{k + 1}, ..., x_{n}; \bm{\phi})dx_1...
dx_{k - 1}...dx_{n}
\end{equation*}


\subsection{Sampling Model}
A {\textbf {sampling model}} is a set of RV $ (X_1, ..., X_{n}) $
with a certain probabilistic structure. The primary objective of the
sampling model is to relate the observed data to the probability model.








\section{A simple statistical model}
Now we can define a generic simple statistical model:
1. Probability model: $ \Phi = \left\{ f(x;\bm{\theta}), 
\bm{\theta} \in \Theta, x \in \mathbb{R}_{X}\right\}  $

















\section{Useful Densities}

{\textbf {Note:}} Different density functions can have very similar
shape with specific parameter. The best way to distinguish between
them is via {\textbf {index measures based on moments}} which are 
invariant to scale and location parameter changes (skewness, kurtosis).

For example, in the case of modeling data referring to exam scores it is
often more realistic to use the {\textbf {Beta}} and {\textbf {not}} the
{\textbf {Normal}} distribution because all scores can be easily
expressed in the interval [0,1]; the Normal distribution has support
$  (- \infty , \infty)  $.





\subsection{Bernoulli}

\begin{table}[h!]
\begin{center}
\begin{threeparttable}
\caption{}			

\begin{tabular}{c|cc}
y & 0 & 1\\
\hline \\[-1.8ex]
$ f(y;\theta) $ & $ 1 - \theta $ & $ \theta $
\end{tabular}


\end{threeparttable}
\end{center}
\end{table}

$ \mathbb{P}(Y = 1) = \theta $, where $ 0 \le  \theta \le  1 $.
Bernoulli density:
\begin{equation*}
		f(y;\theta) = \theta^{y}(1 - \theta)^{1 - y}, 
\end{equation*}
where $ \theta \in [0,1],  y = 0,1$


{\textbf {Mean:}}
\begin{equation*}
\mu(\theta):= E(X) = 0\cdot (1 - \theta) + 1\cdot \theta = \theta
\end{equation*}


{\textbf {Variance:}}
\begin{equation*}
		Var(X) = E(X - E[X])^{2} = (0 - \theta)^{2}(1 - \theta) + (1 -
		\theta)^{2}\theta	 = \theta(1 - \theta)
\end{equation*}




\subsection{Binomial distribution by Bernoulli}
18th century

\begin{equation*}
f(x,\theta) = \binom{n}{x}\theta^{x}(1 - \theta)^{n - x}
\end{equation*}
where $ \theta \in [0,1], x = 0,1, n = 1,2,3,... $
$ \binom{n}{x} = \frac{n!}{(n - x)!x!} $


\subsection{Normal Distribution}
By de Moivre and Laplace in the early 19th century.

\begin{equation*}
f(x;\theta) = \frac{1}{\sigma \sqrt {2 \pi} }exp \left\{ 
		 -  \frac{1}{2 \sigma^{2}}(x - \mu)^{2}
\right\} 
\end{equation*}
where $ \theta:= (\mu,\sigma^{2}) \in \mathbb{R} \times \mathbb{R}_{ + },
x \in \mathbb{R}$

\begin{equation*}
F_{X}(x;\theta) = \int_{ - \infty }^{x} f_{x}(u)du
\end{equation*}

{\textbf {Mean:}}
\begin{align*}
E(X) &= \int_{ - \infty }^{\infty }\frac{1}{\sigma \sqrt {2 \pi}} exp
\left[ 
		 - \frac{(x - \mu)^{2}}{2 \sigma^{2}}
\right] dx\\
 &= \int_{ - \infty }^{\infty }\left( 
		 \frac{\sigma z  + \mu}{\sigma \sqrt {2 \pi}}
 \right)  \text{ exp } \left[ 
  -  \frac{z^{2}}{2}
 \right] (\sigma) dz \\
  &= \left( 
			\frac{\sigma}{\sqrt {2 \pi}}
	\right) \int_{ - \infty }^{\infty } z \text{ exp } \left[ 
	 - \frac{z^{2}}{2}
	\right] dz  + \mu \int_{ - \infty }^{\infty } \left( \frac{1}{
	\sqrt {2 \pi}} 	\right)  \text{ exp } \left[   - \frac{z^{2}}{2} 
	\right] dz\\
	&= 0 + \mu\cdot 1\\
	  &= \mu
\end{align*}
where $ z = \left( \frac{x - \mu}{\sigma} \right)  $ or $ x = 
\sigma z  + \mu$ with $ \frac{dx}{dz} = \sigma $.



{\textbf {Variance:}}
$ Var(X) = E(X^{2}) - (E(X))^{2} = \sigma^{2} $, where
$ E(X^{2}) = \sigma^{2} + \mu^{2} $



\subsection{Uniform Distribution}
\subsubsection{Continuous form:}
\begin{equation*}
f_{x}(x;\theta) = \frac{1}{b - a},
\end{equation*}
where $ \bm{\theta}:=(a,b) \in \mathbb{R}^{2}, a \le x \le b $


\begin{equation*}
F_{X}(x;\theta) = \frac{x - a}{b - a},
\end{equation*}
where $ \bm{\theta}:=(a,b) \in \mathbb{R}^{2}, a \le x \le b $\\


{\textbf {Mean:}}
\begin{equation*}
		\mu(\theta) = E[X] = \int_{\theta_1}^{\theta_2} \frac{x}{(\theta_2
		 - \theta_1)}dx = \frac{1}{2} \frac{1}{(\theta_2 - \theta_1)}x^{2}
		 \Bigg|_{\theta_2}^{\theta_1} = \frac{\theta_1  + \theta_2}{2}
\end{equation*}

\subsubsection{Discrete form:}
\begin{equation*}
f_{x}(x;\theta) = \frac{1}{\theta + 1},
\end{equation*}
where $ \theta $ is an integer, $ x = 0,1,2,...,\theta $. $ \theta $ is
the maximum value of $ x $.
For example, if $ \theta = 9 $, then $ x $ can be 0,1,2,3,4,5,6,7,8,9.
The density would be
\begin{equation*}
f_{x}(x;9) = \frac{1}{9 + 1} = \frac{1}{10} = 0.1
\end{equation*}
\begin{equation*}
F_{X}(x;\theta) = \frac{x + 1}{\theta + 1},
\end{equation*}
where $ \theta $ is an integer, $ x = 0,1,2,...,\theta $


\begin{figure}[H]
		\center{\includegraphics[scale =.5 ]  {figures/discrete_uniform_pdf.png}}
		\includegraphics[scale = .5]  {figures/discrete_uniform_cdf.png}
\end{figure}




\subsection{Poisson (discrete)}
\subsubsection{Continuous:}
\begin{equation*}
f_{x}(x;\theta) = \frac{e^{ - \theta}\theta^{x}}{x!},
\end{equation*}
where $ \theta >0, x = 0,1,2,3,... $

{\textbf {Mean:}}
\begin{equation*}
\mu(\theta):=E(X) = \sum\limits_{k = 0} ^\infty k \left( 
		\frac{e^{ - \theta}\theta^{k}}{k!}
\right) = \theta e^{ - \theta} \sum\limits_{k = 0} ^\infty 
\frac{\theta^{k - 1}}{(k - 1)!} = \theta, \text{ since }
\sum\limits_{k = 0} ^	\infty \frac{\theta^{k - 1}}{(k - 1)!} =e^{\theta}
\end{equation*}


\subsubsection{Discrete}
\begin{equation*}
F_{X}(x;\theta) = \sum\limits_{k = 0} ^x \frac{e^{ - \theta}\theta^{x}}{
k!}	,
\end{equation*}
where $ \theta >0, x = 0,1,2,3,... $



\subsection{Exponential Distribution}

DF:
\begin{equation*}
F_{X}(x;\theta) = 1 - e^{ - \theta x}, 
\end{equation*}
where $ \theta > 0, x \in \mathbb{R}_{ + }:= [0, \infty ]$

PDF:
\begin{equation*}
f_{x}(x;\theta) = \theta e^{ - \theta x}, 
\end{equation*}
where $ \theta > 0, x \in \mathbb{R}_{ + } $


\subsection{Cauchy distribution}
Cauchy distribution $ C(\alpha,\beta) $ has no moments.
\begin{equation*}
F(x;\alpha,\beta) = \frac{1}{2} + \left( 
		\frac{1}{\pi}tan^{ - 1}\left( 
				\frac{x - \alpha}{\beta}
		\right) 
\right) 
\end{equation*}
\begin{equation*}
f(x;\alpha,\beta) = \frac{1}{\beta \left( 
				1  + \left( 
						\frac{x - \alpha}{\beta}
				\right) ^{2}
\right) }
\end{equation*}
where $ \alpha \in \mathbb{R}, \beta \in \mathbb{R}_{ + }, x \in 
\mathbb{R}$.













\end{document}

